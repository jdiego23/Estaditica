{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las redes neuronales\n",
    "Una neurona es:\n",
    "Una multiplicacion entre la entrada, y los coeficientes + un bias a la que se le aplica una funcion no lineal en la salida, una de esas funcione es \\sigmoid que es la utilizada en la regresion logistica.\n",
    "\n",
    "\\$a^{(L)} = \\sigma(W^{(L)}.a^{(L-1)} + b^{(L)})\\$\n",
    "\n",
    "$W^{L(1)}$ es la matriz the coeficientes, quede es de tama√±o nxm. Donde n es la dimension de los datos de entrada y m es la dimension de los datos de salida.\n",
    "\n",
    "Por ejemplo en la siguiente imagen tenemos:\n",
    "<img src=\"img/Intro_nnet.png\">\n",
    "\n",
    "\n",
    "\n",
    "\\$a^{(1)} = \\sigma(W^{(1)}.a^{(0)} + b^{(1)})\\$\n",
    "\n",
    "\\$a^{(2)} = \\sigma(W^{(2)}.a^{(1)} + b^{(2)})\\$\n",
    "\n",
    "\\$a^{(3)} = \\sigma(W^{(3)}.a^{(2)} + b^{(3)})\\$\n",
    "\n",
    "\\$a^{(1_0)} = \\sigma(W^{(0_1)}.a^{(0_0)} + W^{(1_1)}.a^{(0_1)} + W^{(2_1)}.a^{(0_2)} + b^{(2)})\\$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activaciones\n",
    "\n",
    "Las activaciones permiten que las redes neuronales puedan aproximar cualquier funcion continua con 2 layers y con 3 cualquier funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAEoCAYAAADPMiGcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VOX1+PHPSUISliTshD3suyAEcF/qBmpFa624AW6orbWt7a/VqrXVLtavrV1csaIgCrhLFfelVquSgOz7npAAgYQEyJ45vz/mBoc4IdvM3FnO+/Wa18zc+8zck0lOztx7n/s8oqoYY4wxsSbO7QCMMcYYN1gBNMYYE5OsABpjjIlJVgCNMcbEJCuAxhhjYpIVQGOMMTHJCqA5JhG5WUQ+cDsOY9wiIleJyHvhtl0R+UREbghlTNHGCmAUEJFDPjePiJT5PL/K7fiMiQQicoqI/E9EikWkUEQ+F5Hxqvq8qp4b6njc2m4sSXA7ANNyqtqu9rGIbAduUFXbazOmkUQkFXgTuAV4EUgETgUq3IzLBJftAcYAETlZRL5yvtnmicjDIpLgrEsWERWRG0Vki4gUicjD334L+YeIHHDanO3Cj2FMMA0GUNX5qlqjqmWq+p6qrhSRGSLyWW1DETlXRDY4+fSYiPyn9lCk0/ZzJ8cOiMhWETnJWZ4jIntFZLrPe6WJyFwRKRCRHSJyt4jE+byX73bPEZH1znYfASRkn06UsgIYG6qAW4GOeL/Vfheoe+5gMnA8MBa4VkTO8Fl3GpANdAIeAf4V5HiNCbWNQI2IzBGRySLSwV8jEekMvAzciTcfNgAn1Wk2EVjprH8BWACMBwYCVwOPiEjtUZt/AmlAf+B0YBpwbT3bfQW4G+gMbAFObu4Pa7ysAMYAVV2iqlnON9steAvY6XWa/VFVS1R1G/ApMMZn3QZVnauqNcAcoK+ItA9N9MYEn6qWAKcACjwFFIjIIhHpVqfp+cAaVX1VVauBfwC767TZpqrPOPmyEOgN3KeqFar6HlAJDBSReOBy4E5VPaiq24G/ANf4CfF8YK2qvqyqVcDf/GzXNJEVwBggIsNF5G0R2SMiJcBv8H6L9OWbTKVAu2Oso856YyKeqq5T1Rmq2gsYCfTAW2h89QByfF6jQG6dNnt8Hpc57eoua4c3BxOBHT7rdgA9/YTnb7s5ftqZJrACGBueApYBA1Q1FbgPO39gTL1UdT3wLN5C6Csf6FX7RETE93kT7cN7eqKvz7I+wC4/bfPx7kn6bre3n3amCawAxoYUoFhVD4nICOBGtwMyJpyIyFAR+bmI9HKe9wauAL6s0/QtYJSIXOx0JPsRkN6cbTqHSF8E/iAiKSLSF7gdmOen+VvACBH5nrPd25q7XfMNK4Cx4WfADSJyCHgU73kJY8w3DuLtvPKViBzGW/hWAz/3baSq+4DLgAeB/cBwvB3Emnu5xI+Bw8BW4DO8nWZm123ks90HnO0OAj5v5jaNQ2xCXGOMaR7nkoVc4CpV/djteEzT2B6gMcY0gYicJyLtRSQJ+DXe8+l1D5WaCGAF0BhjmuZEvNfh7cN7Te3FqlrmbkimOewQqDHGmJhke4DGGGNikhVAY4wxMSlsZ4Po3LmzZmRkuB2GMS22dOnSfaraxe046mO5ZqJBc/IsIAVQRGYDFwJ7VbXuyAm1oxb8He94dqXADFVddqz3zMjIIDs7OxDhGeMqEdnRcKuj2jc7n5yZBu52mv5eVec0tD3LNRMNmppnELhDoM8Ck46xfjLeCzcHATOBxwO0XWOi0bM0I59EpCNwL94LuicA99Y3q4ExJkB7gKr6qYhkHKPJFGCuM4Drl841NN1VNT8Q2zeRpcajVNV4qKzxUFVde6/ee+dW41EU8HZSVmo7K9cuC6feywO7tqNTu6SAvV9z8wk4A3hfVQsBROR9vIV0fsCCM8aHqlJR7c3X6hql2uN9XOVRamqUKue5RxWPB9TJZY9+c+/xk+Pe9/5mGwB9O7UlPS05oPGH6hxgT44euTzXWWYFMIpVVNewZe9hNu45yIY9B9nk3OcURtclU49fNZbJo7qHcpP15VN9y79FRGbi3XukT58+wYnSRJTqGg/5xeXkFpWRW1RKblEZ+cVlHKqo5nBFDaWVPveVNZRWVFNaVUOovov+7qIRTD8pI6DvGaoC6G/mgW99bJaUke+1r3N5f+0eNuw+yPb9pdR4v96RECcM6NKO0b3ac8mYnrRJSqBVfByJ8UKr+DjvLcH7PCEujvh475+MACLi3OMsE0TCZzqLwekpod5kffnUqDwDUNVZwCyAzMzM8NmdNiGRX1zGu6t3s2pXyZFit7uk/Ei+gjffuqYkkZrcijZJCbRNjKdH+0TaJMbTNimeNoneZUmt4kmIExLi45x78d7HxZEQL8THCfHi5KwIcU4+x8X55rYc+eP15rYceQzeP+x+XdoG/HMIVQHM5eipO3oBeXUbWVJGthe+2smvX1tFrw6tGdEjlfNHdWdwtxSGpKeQ0aktiQl21U2A1JdPuXgPg/ou/yRkUZmwllNYytur81m8ajfLcw4A0C01iT4d2zChX0d6dWjt3NrQq0Nruqe1jvqcDVUBXATcKiIL8J6gL7bzf9Hl/bV7uPv1VZw5pAuzpmXSKj66E8dlfvNJRN4F/ujT8eVc4E63gjTu21JwiLdX5fP26t2sySsBYGTPVP7feUOYNDKdAV1ie17rQF0GMR/vN8/OIpKLtydaKwBVfQJYjLfL9ma83bavDcR2TXhYuqOIH89fxqhe7Xn0qrFW/FqoufmkqoUicj+Q5bzVfbUdYkxsWbqjkLteW8363QcBOL5Pe359/lAmj+xO745tXI4ufASqF+gVDaxXvBNHmiizee8hrp+TRfe01syenkmbxLAdWyFitCSfVHU2fuaTM7FBVZn31U7u+/cauqe15rffHc55I9Ppntba7dDCkv23Ms22p6Sc6bOXkBAXx5xrJwT0UgBjTNOUV9Vwz+ureWlpLmcO6cLfLj+etDat3A4rrFkBNM1SUl7F9NlLOFBaycKbTqRPJzusYoxbdh0o45Z5S1mZW8xt3xnIT88eTFxcuPSTDl9WAE2TVVTXcNPcpWzee4hnrh3PyJ5pbodkTMz635Z93PrC11RWe5h1zTjOHZHudkgRwwqgaRKPR/n5iyv4Yut+Hr58NKcOCtsxno2JaqrKv/67jT+9vY7+Xdrx5DXjYr5XZ1NZATRN8ofF63hzZT53TB7KJcf3cjscY2JSaWU1v3plFf9ekcekEek89IPRtEuyf+dNZZ+YabT31uzm6c+2MeOkDG46rb/b4RgTkzwe5bpns1iyrZBfThrCLacPQMTO9zWHFUDTaB+t30tqcgL3XDjcEs4Yl7yyLJcvtxbyx0tGceVEGzKyJeyKZdNoWdsLyczoSLz1LjPGFcWlVTzw9nrG9mnP1PG9G36BOSYrgKZR9h+qYEvBYcZndHQ7FGNi1kPvbaCotJL7Lx5plzkEgBVA0yhZ24sAmNDP5lc1xg2rcouZ99UOpp2YwYgedulRIFgBNI2Stb2QpIQ4RvVs73YoxsQcj0e5+43VdGqbxO3nDnY7nKhhBdA0Stb2Qsb0bh/106MYE44WZOWwIucAd10wlNRkG94sUOy/mWnQ4Ypq1uSV2Pk/Y1xQeLiSB99dz8R+Hbl4TE+3w4kqVgBNg5btLKLGo4zvZwXQmFB78J31HCqv5v6LR9rlRwEWkAIoIpNEZIOIbBaRO/ysnyEiBSKy3LndEIjtmtDI2lZInMDYPnb+LxQakU8P++TSRhE54LOuxmfdotBGbgJt2c4iFmTlcN0p/RjcLcXtcKJOiy+EF5F44FHgHCAXyBKRRaq6tk7Thap6a0u3Z0JvyfZChvdIJcXOPQRdY/JJVX/m0/7HwPE+b1GmqmNCFa8JnhqPcs/rq0lPTeYnZw1yO5yoFIg9wAnAZlXdqqqVwAJgSgDe14SBymoPy3MO2Pm/0GlqPl0BzA9JZCak5n25gzV5Jdxz4XDa2jifQRGIAtgTyPF5nussq+tSEVkpIi+LiA1hECFW5xVTXuVhghXAUGlsPiEifYF+wEc+i5NFJFtEvhSRi4MXpgmmgoMVPPTeBk4Z2JnzR9n0RsESiALo76ys1nn+byBDVY8DPgDm+H0jkZlO8mYXFBQEIDTTUlnbCgHItAIYKo3Jp1pTgZdVtcZnWR9VzQSuBP4mIgP8bsRyLaz96e11lFfV8LspI6zjSxAFogDmAr57dL2APN8GqrpfVSucp08B4/y9karOUtVMVc3s0sXmmQsHWdsL6d+5LV1SktwOJVY0mE8+plLn8Keq5jn3W4FPOPr8oG87y7UwtTznAK8u28XM0/rb/H5BFogCmAUMEpF+IpKINymP6n0mIt19nl4ErAvAdk2QeTxK1vYiO/8XWg3mE4CIDAE6AF/4LOsgIknO487AyUDdzmgmzM37cgftkhL44RkD3Q4l6rX4zKqqVovIrcC7QDwwW1XXiMh9QLaqLgJuE5GLgGqgEJjR0u2a4Nu09xDFZVV2/V8INTKfwNv5ZYGq+h4eHQY8KSIevF9uH/DTG9uEsZLyKt5cmcf3xvayji8hEJBPWFUXA4vrLPuNz+M7gTsDsS0TOku2e8//jc+wAbBDqaF8cp7/1s/r/geMCmpwJqgWLc+jvMpjUx2FiI0EY+qVta2QrilJ9OnYxu1QjIkJC7NyGNY9lVE9bbaHULACaPxSVbK2FzK+X0frhWZMCKzeVcyqXcVcMaG35VyIWAE0fu06UEZ+cbld/2dMiCzMyiEpIY4po23A61CxAmj8yjpy/s8KoDHBVlZZw+vLd3H+qO6ktbEhB0PFCqDxa8m2IlKSExiSbgPwGhNsi1flc7C8msut80tIWQE0fmVtLySzbwfi4+xchDHBtjArh36d2zLRLjkKKSuA5lsKD1eyee8hu/7PmBDYUnCIJdsLuXy8dX4JNSuA5ltqz/9ZBxhjgm9hVg4JccKlY3u5HUrMsQJoviVrWyGJCXGM6mXXIhkTTJXVHl5ZmsvZw7rZeLsusAJoviVreyFjerUnKSHe7VCMiWofrtvD/sOVXD7BOr+4wQqgOcrhimpW55Uwvp8Nf2ZMsM3PyqFHWjKnDbIZOdxgBdAcZXnOAWo8atf/GRNkuUWl/HdTAZdl9rbe1i6xAmiOsmRbIXEC4/raHqAxwfRSdi4Al2Va5xe3WAE0R8naXsiw7qmkJNtoFMYES41HeSk7h1MHdaFXBxts3i1WAM0RVTUevt55wA5/GhNkn24qIK+4nCts5BdXBaQAisgkEdkgIptF5A4/65NEZKGz/isRyQjEdk1grd5VTFlVDRPsAnhXNSKfZohIgYgsd243+KybLiKbnNv00EZuGmvhkhw6tU3krGHd3A4lprW4AIpIPPAoMBkYDlwhIsPrNLseKFLVgcDDwJ9bul0TeDYAtvsamU8AC1V1jHP7l/PajsC9wERgAnCviNjJ3DBTcLCCD9bt4dJxvUhMsINwbgrEpz8B2KyqW1W1ElgATKnTZgowx3n8MnCW2Jg/YWfJtiL6dW5rF+S6qzH5VJ/zgPdVtVBVi4D3gUlBitM00yvLcqn2qA18HQYCUQB7Ajk+z3OdZX7bqGo1UAx0CsC2TYB4PMrSHd4BsI2rGpNPAJeKyEoReVlEav+TNva1xiWqysKsHMZndGBAl3ZuhxPzAlEA/e3JaTPaICIzRSRbRLILCgoCEJpprM0FhygqrbIBsN3XmFz5N5ChqscBH/DN0ZVG5RlYrrnlq22FbNt3mKnj+7gdiiEwBTAX8N2X7wXk1ddGRBKANKCw7hup6ixVzVTVzC5dbGSEULIBsMNGg/mkqvtVtcJ5+hQwrrGv9XkPyzUXLMzKISU5gfNHdXc7FENgCmAWMEhE+olIIjAVWFSnzSKgtkfa94GPVNXvN1PjjqxthXRJSaJvJ7smyWUN5pOI+P73vAhY5zx+FzhXRDo4nV/OdZaZMFBcWsXiVflcPKYnrRNtnN1wkNDSN1DVahG5FW+ixQOzVXWNiNwHZKvqIuBp4DkR2Yx3z29qS7drAitrexETMjrafGQua2Q+3SYiFwHVePNphvPaQhG5H28RBbhPVb91pMW44/Xlu6io9ljnlzDS4gIIoKqLgcV1lv3G53E5cFkgtmUCb9eBMnYdKOPGU/u5HYqhUfl0J3BnPa+dDcwOaoCmyVSV+Ut2MqpnGiN72jRj4cIuQjFkbXOu/7MOMMYExapdxazffdD2/sKMFUDDku2FpCQlMDQ91e1QjIlK85fk0LpVPBeN6eF2KMaHFUBD1rZCxmV0sClZjAmCwxXVLFq+iwuO606qDTIfVqwAxriiw5Vs2nvIhj8zJkjeWpXP4coaptrhz7BjBTDG2fifxgTXgiU7Gdi1nc2xGYasAMa4rO2FJMbHcVwv65lmTKBt3HOQZTsPMHV8b7vEKAxZAYxxWduLGN07jeRWdmGuMYG2MCuHVvHCJcfbkKzhyApgDCutrGb1rmI7/GlMEFRU1/DqslzOHZFOp3Y2w0o4sgIYw5bvPEC1R+36P2OC4L01eygqrbLOL2HMCmAMW7K9EBHs5LwxQbAwK4deHVpz8oDObodi6mEFMIZlbS9kWHqqXZtkTIDt3F/KZ5v3cXlmb+Ls+tqwZQUwRlXVeFi24wAT7PCnMQH3YnYOcQLfz+zldijmGKwAxqg1eSWUVdVYBxhjAqy6xsNLS3M4Y0hXuqe1djsccwxWAGPUkQGwM+z8nzGB9MmGAvaUVFjnlwhgBTBGLdleSN9Obeiamux2KMZElQVZOXRJSeLMoV3dDsU0oEUFUEQ6isj7IrLJufe7OyEiNSKy3LnVnS3ehJjHo2RvL7TDn2FKRCaJyAYR2Swid/hZf7uIrBWRlSLyoYj09VlnueaiPSXlfLxhL98f14tW8bZ/Ee5a+hu6A/hQVQcBHzrP/SlT1THO7aIWbtO00NZ9hygqrWKCFcCwIyLxwKPAZGA4cIWIDK/T7GsgU1WPA14GHvRZZ7nmokc+2oyq2uHPCNHSAjgFmOM8ngNc3ML3MyGwZFsRYBPghqkJwGZV3aqqlcACvHl2hKp+rKqlztMvAetqGAZW5RYz76sdTDsxg76d2rodjmmElhbAbqqaD+Dc13fQO1lEskXkSxGpt0iKyEynXXZBQUELQzP1ydpeSOd2SWR0auN2KObbegI5Ps9znWX1uR542+e55ZoLPB7l7jdW06ltErefO9jtcEwjJTTUQEQ+ANL9rLqrCdvpo6p5ItIf+EhEVqnqlrqNVHUWMAsgMzNTm/D+pgmWbCtkQr8ONjp9ePL3S/GbCyJyNZAJnO6z2HLNBQuycliRc4CHLx9tA0tEkAYLoKqeXd86EdkjIt1VNV9EugN763mPPOd+q4h8AhwPfCspTfDlHShj14Eybji1n9uhGP9yAd8TSL2AvLqNRORsvF9CT1fVitrllmuhV3i4kgffXc/Efh25eIzN+hBJWnoIdBEw3Xk8HXijbgMR6SAiSc7jzsDJwNoWbtc0k02AG/aygEEi0k9EEoGpePPsCBE5HngSuEhV9/ost1xzwZ/fXs+h8mruv3ikHVWJMC0tgA8A54jIJuAc5zkikiki/3LaDAOyRWQF8DHwgKpaUrpkybZC2iUlMKx7qtuhGD9UtRq4FXgXWAe8qKprROQ+Eant1fl/QDvgpTqXO1iuhdiynUUszM7hulP6MbhbitvhmCZq8BDosajqfuAsP8uzgRucx/8DRrVkOyZwsrYXMrZvB+JtgN6wpaqLgcV1lv3G57Hf0xKWa6FV41HueX016anJ3HbWILfDMc1gV2rGkKLDlWzcc4gJNvyZMS0278sdrMkr4e4Lh9EuqUX7EsYlVgBjSPYO5/o/O/9nTIsUHKzgofc2cMrAzlwwqrvb4ZhmsgIYQ7K2F5IYH8fo3u3dDsWYiPanxesor6rhd1NGWMeXCGYFMIZkbS/kuF5pJLeKdzsUYyLWV1v38+rXu5h5Wn8GdGnndjimBawAxoiyyhpW5Rbb8GfGtEBVjYd73lhNz/atufVM6/gS6awAxoivc4qo9qgNgG1MCzz7+XY27jnEvd8dTutEO5IS6azrUgyoqK7h7x9sIjEhjrF9rQeoMU2lqjz92TYeeGc93xnalXOGd3M7JBMAVgCjnMej3P7iCr7aVsjfp44hrbWNU2hMU5RWVnPHK6tYtCKP80Z046HLRlvHlyhhBTCKqSr3v7WWt1bm8+vzhzLFxik0pkl27D/MTc8tZcOeg/y/84bwwzMGWPGLIlYAo9isT7fyzOfbue7kftx4an+3wzEmony8YS8/mf81IsKz107g9MFd3A7JBJgVwCj16rJc/vT2ei48rjt3XzDMvrUa00gej/Lox5v56wcbGZqeypNXj6OPzZ0ZlawARqFPNxbwy5dXcmL/TvzlB6OJs3E/jWmUg+VV3P7iCt5fu4cpY3rwwPeOs96eUcwKYJRZvauYW+YtZWDXdjw5bRxJCZa8xjSktLKaTzYU8NB7G9ixv5R7LhzOdSdn2JGTKGcFMIrs3F/KjGeW0L5NInOum2AzUxtzDCXlVXy0bi9vr87nPxsLKK/y0C01iXnXT+TEAZ3cDs+EQIsKoIhcBvwW7zxkE5xpkPy1mwT8HYgH/qWqD7Rku+bb9h+qYNrsr6iqURbMHE+31GS3QzLN1FC+OJPezgXGAfuBy1V1u7PuTuB6oAa4TVXfDWHoYa/ocCXvr9vDO6t389mmfVTWeOiaksQPMnszaWQ6EzI6khBv44PEipbuAa4Gvod3dmq/RCQeeBTvhLm5QJaILLKJOlvGO7XRQTbuOciGPQf5bNM+8ovLeeHGiQzsahNzRqpG5sv1QJGqDhSRqcCfgctFZDjeGeRHAD2AD0RksKrWhPancFdVjYf8A+XkFpWSW1R25H5HYSnLcw5Q41F6tm/NNSf25fxR6Rzfu4OdJ49RLZ0Qdx3Q0HHyCcBmVd3qtF0ATAGaXQCrajwUHa5s7ssDShvTRkFR5752mfd57fpqj4eqGqWqxkNljYeq6qOf187lV1vwCg5WHHn/lOQEhnRL4bcXjWBcXxvqLMI1Jl+m4D3yAvAy8Ih4k3AKsEBVK4BtIrLZeb8vmhNIjUfZd6jiqGXq5w++9m/b+7i2nR7VVhU8qijOvbPe4+RGjcd7q6rx3ld7PFQfeaxU13goq6rhcGUNpRXVR99XVnO4oobiskp2FZWxu6Qcj8+24wS6p7WmZ/vWzDytP5NHpjOqZ5qd3zMhOQfYE8jxeZ4LTGzJG67PP8h3H/msRUFFouRWcQzulsLpg7swpFsKg7q1Y0h6CumpyZbM0aMx+XKkjapWi0gx0MlZ/mWd1zZ79IN9hyqY+McPm/vyoGuTGE+bxATaJnnvU5MTOGFAJ3p1aEOvDq3p1aE1vTu0IT0tmVZ2WNP40WABFJEPgHQ/q+5S1TcasQ1//5n97jiJyExgJkCfPn3qfcMe7ZP5wyUjG7Hp0BC/P2KdNuL9ILz3UvtCZ5nQKl5oFR/n3ITEhDgSjzyPIyU5gZ7tW9uhmujXmHypr01Acy0lOYE/XjLKz2v9vJ/Pct+/79q7OBFEvrkXEeKcXIiTb3IgPk5IiIsjIV5IiPM+bxUfR3yc0LpVPG2S4mmbmEDrVvGWC6bFGiyAqnp2C7eRC/T2ed4LyKtnW7OAWQCZmZn1Hl3s1C6Jqyb2bWFYxoSlxuRLbZtcEUkA0oDCRr4WaFyutUlM4MqJ9X8RNSbSheK4QBYwSET6iUgi3pP0i0KwXWMiUWPyZREw3Xn8feAjVVVn+VQRSRKRfsAgYEmI4jYm4rSoAIrIJSKSC5wIvCUi7zrLe4jIYvCeowBuBd4F1gEvquqaloVtTHSqL19E5D4Ruchp9jTQyenkcjtwh/PaNcCLeDvMvAP8KNZ6gBrTFKL+unWFAREpAHY00KwzsC8E4bRUpMQJkRNrpMQJMERVw/balEbkWqR81pESJ1iswdDkPAvbkWBUtcGh10UkW1UzQxFPS0RKnBA5sUZKnOCN1e0YjqWhXIuUzzpS4gSLNRiak2fWN9gYY0xMsgJojDEmJkV6AZzldgCNFClxQuTEGilxQmTF6k+kxB8pcYLFGgxNjjNsO8EYY4wxwRTpe4DGGGNMs1gBNMYYE5MirgCKyGUiskZEPCKSWWfdnSKyWUQ2iMh5bsXoj4j8VkR2ichy53a+2zH5EpFJzue2WUTucDueYxGR7SKyyvkcw+oSAxGZLSJ7RWS1z7KOIvK+iGxy7ju4GWNjWJ4Fh+VZYAQqzyKuAPLNHISf+i6sMxfaJOAxZ261cPKwqo5xbovdDqaWzxx0k4HhwBXO5xnOznQ+x3C7PulZvH9/vu4APlTVQcCHzvNwZ3kWYJZnAfUsAciziCuAqrpOVTf4WXVkLjRV3QbUzoVmGnZkDjpVrQRq56AzTaSqn+IdmNrXFGCO83gOcHFIg2oGy7OgsDwLkEDlWcQVwGPwN49as+dCC5JbRWSls/seTofBIuGz86XAeyKy1JnWJ9x1U9V8AOe+q8vxtEQk/K1YngVG1OdZWA6F1sw5CBs9F1qwHCtu4HHgfiem+4G/ANeFLrpjcv2za6KTVTVPRLoC74vIeucboWkCy7OQc/2za6Koz7OwLIDNnIOw0XOhBUtj4xaRp4A3gxxOU7j+2TWFquY593tF5DW8h5bCOTH3iEh3Vc0Xke7AXrcDAsszF7j+2TVFLORZNB0CDeu50JxfSK1L8HYyCBcRM2ejiLQVkZTax8C5hNdn6Y/v/H3Tgfr2riKB5VnzWZ4FV9PzTFUj6ob3jzoXqAD2AO/6rLsL2AJsACa7HWuduJ8DVgErnV9Ud7djqhPf+cBG5/O7y+14jhFnf2CFc1sTbrEC84F8oMr5O70e6IS3V9om576j23E24uewPAtOfJZngYkvIHlmQ6EZY4yJSdF0CNQYY4xpNCuAxhhjYpIVQGOMMTHJCqAxxpiYZAXQGGNMTLICaIwxJiZZATTGGBOTrAAaY4yJSVYAjTHGxCQrgMYYY2KSFUCDiJwhIrlux2FMKIjIdhFpzkwYdd/nWRH5fSBiauIyghT1AAAeY0lEQVR2LV8DxApgFHESu0xEDonIbidB27kdlzHGhCMrgNHnu6raDhgDHA/c6XI8xpgmEJF4t2OIFVYAo5Sq7gbexVsIceZve0hEdorIHhF5QkRa+3utiKiIDPR57sqhHmOCTUTiROQOEdkiIvtF5EUR6eiz/iXnaEqxiHwqIiPqeZ8UEflYRP4hIuOdHEvwWX+piCyv57XPisjjIrJYRA4DZ1q+hoYVwCglIr2AycBmZ9GfgcF4C+JAoCfwG3eiMyZs3AZcDJwO9ACKgEd91r+Nd9LfrsAy4Pm6byAitfPQfa6qt6lqFrAfOMen2dV45yqsz5XAH4AU4DMsX0PCCmD0eV1EDgI5wF7gXhER4EbgZ6paqKoHgT/inZHamFh2E97JXnNVtQL4LfD92r03VZ2tqgd91o0WkTSf1/cA/gO8pKp3+yyfg7fo4exRnge8cIw43lDVz1XVg3cSYsvXEEhouImJMBer6gcicjrehOsMJAJtgKXeWgiAAHauwcS6vsBrIuLxWVYDdBOR3Xj3yi4DugC1bToDxc7jC4BDwBN13ncesM7phPYD4L+qmn+MOHJ8HnfB8jUkbA8wSqnqf4BngYeAfUAZMEJV2zu3NKezjD+leBOwVnpQgzXGPTnAZJ+8aK+qyaq6C+9hySnA2UAakOG8Rnxe/xTwDrBYRNrWLnRe/wVwCXANxz78CaA+jy1fQ8QKYHT7G97zEMfhTdSHRaQrgIj0FJHz6nndcuBKEYkXkUl4z48YE42eAP4gIn0BRKSLiExx1qXgPRy5H2+B+WM973ErsAF4s05HlbnAL4FRwGuNDcg5DGr5GgJWAKOYqhbgTcJ7gF/h7RDzpYiUAB8AQ+p56U+A7wIHgKuA14MfrTGu+DuwCHjPOXf+JTDRWTcX2AHsAtY6675FVRWYiXdv8g0RSXZWvYZziFVVDzcxLsvXEBDv784YY0ygicgW4CZV/cDtWMy32R6gMcYEgYhcivfc3kdux2L8s16gxhgTYCLyCTAcuMY5p2fCkB0CNcYYE5PsEKgxxpiYFLaHQDt37qwZGRluh2FMiy1dunSfqnZxO476WK6ZaNCcPAvbApiRkUF2drbbYRjTYiKyw+0YjsVyzUSD5uSZHQI1xhgTk6wAGtMC1TXWwc+YYAtWnlkBNKaZcotKOefhT/nfln1uh2JM1Np3qILz//Ff3lm9O+DvbQXQmGYoraxm5tyl7DtUQbfU5IZfYIxpskMV1Vz7TBY7C0vpkpIU8PcP204wxoQrVeX/vbSSdbtLmD1jPAO61DdIvzGmuSqrPdwybylr80t4ato4xvXtEPBt2B6gMU302CdbeGtVPr+aNJQzh3R1Oxxjoo7Ho/zipRX8d9M+HvjeKL4ztFtQtmMF0Jgm+GDtHh56bwNTxvTgptP6ux2OMVFHVbn/rbUsWpHHryYN5bLM3kHblhVAYxpp056D/HThckb2SOPPlx6Hz2zdxpgAeeI/W3nm8+1cd3I/bj49uF8yrQAa0wgHSiu5YW42ya3imTVtHMmt4t0OyZio82J2Dn9+Zz0Xje7B3RcMC/qXTCuAxjSgusbDj+d/Td6BMp68Zizd01o3/CJjTJN8uG4Pd766ilMHdeahy0YTFxf8IywhLYAi8jMRWSMiq0Vkvs/MycaErT+9vZ7/btrH7y8eybi+Hd0Ox5ios3RHET96YRkjeqTy+NXjSEwITWkKWQEUkZ7AbUCmqo4E4oGpodq+Mc3x8tJcnv5sGzNOyuDy8X3cDseYqLNpz0GuezaL9NRkZs8YT7uk0F2dF+pDoAlAaxFJANoAeSHevjGN9vXOIn796ipOGtCJuy4Y5nY4xkSdvANlTJu9hMSEOJ67fiKd2wX+YvdjCVkBVNVdwEPATiAfKFbV93zbiMhMEckWkeyCgoJQhWbMt+wpKeem55bSLS2JR68cS6t4d0+Xi8gQEVnucysRkZ/WaXOGiBT7tPmNW/Ea05ADpZVMn72EQ+XVPHvteHp3bBPyGEJ5CLQDMAXoB/QA2orI1b5tVHWWqmaqamaXLmE7fZqJcuVVNcx8bimHKqp5alomHdomuh0SqrpBVceo6hhgHFAKvOan6X9r26nqfaGN0pjGKaus4fo52ezYX8qsaZmM6JHmShyh/Fp7NrBNVQtUtQp4FTgphNs3pkGqyq9fXcWKnAP89QdjGJqe6nZI/pwFbFHVsJ5n0Bh/qms83PrCMpbtLOLvU8dw4oBOrsUSygK4EzhBRNqI9+KOs4B1Idy+MQ16+rNtvPr1Ln569iAmjUx3O5z6TAXm17PuRBFZISJvi8iIUAZlTENUlTtfXcWH6/dy/5SRTB7V3dV4QnkO8CvgZWAZsMrZ9qxQbd+YhvxnYwF/XLyOySPTue07g9wOxy8RSQQuAl7ys3oZ0FdVRwP/BF4/xvvY+XYTcv/37gZeWprLT84axNUn9HU7nND2AlXVe1V1qKqOVNVrVLUilNs3pj7b9h3mxy8sY3C3lJBdhNtMk4Flqrqn7gpVLVHVQ87jxUArEens703sfLsJtWc+38Zjn2zhyol9+OnZ4fEF00aCMTHvYHkVN87NJj5OeGpaJm1DeB1SM1xBPYc/RSTdOb2AiEzAm9/7QxibMX4tWpHHfW+uZdKIdO6fMjJsxtEN60w3JthqPMpPFyxn277DPHf9BFe6YjeWiLQBzgFu8ll2M4CqPgF8H7hFRKqBMmCqqqobsRpT67NN+/j5i8sZn9GRv00dQ3wYHV2xAmhi2l/f38CH6/dy35QRnDTA79HCsKGqpUCnOsue8Hn8CPBIqOMypj6rcou56blsBnRpx1PTMsNuEHk7BGpi1r9X5PHox1uYOr4314TBCXljosm2fYeZ8cwS2rdJZM51E0hr3crtkL7FCqCJSat3FfP/Xl5BZt8O3BdG5ySMiQZ7D5YzbfZXKPDc9RPolhqe8x5YATQxp+BgBTPnZtOhTWJIR543JhaUlFcxfXYW+w9VMnvGePp3aed2SPWyc4AmplRWe/jh80vZf7iSl28+iS4poR1815hoVl5Vw8y52Wzac5CnZ4xnTO/2bod0TFYATcxQVe5dtIas7d4hmEb1cmf8QWOiUY1Huf3F5Xy5tZC/XT6G0weH//WlduzHxIx5X+5g/pKd3HLGAKaM6el2OMZEDVXld/9ew+JVu7n7gmFcfHxk5JcVQBMTvtiyn9/9ey3fGdqVX5w7xO1wjIkqj3y0mblf7OCm0/pzw6n93Q6n0awAmqiXU1jKj15YRt9ObcLuQlxjIt38JTv5y/sb+d7Ynvxq0lC3w2kSK4Amqh2uqObGudlU1Xh4alomqcnhdy2SMZHq3TW7ueu1VZwxpAt/vvS4cB5D16+QFkARaS8iL4vIehFZJyInhnL7JraoKr94aQUb9xzkkSvHhnV3bGMizVdb9/Pj+V9zXK/2PHbVWFrFR97+VKh7gf4deEdVv+9M6xK+Ay+aiPfPjzbz9urd/Pr8oRHRI82YSLF+dwk3zM2md4fWPDNjPG0SI/OCgpBFLSKpwGnADABVrQQqQ7V9E1veW7Obv76/kUuO78mNEXRS3phwl1NYyrSnl9A2MYG510+kQ9tEt0NqtlDus/YHCoBnRORrEfmXiLT1bWCTdJpA2LD7ID9buJzRvdL40/dG2TBnxgRI4eFKps9eQnlVDXOum0DP9q3dDqlFQlkAE4CxwOOqejxwGLjDt4FN0mlaquhwJTfOzaZNUgJPXhN+o88bE6kOV1Rz7bNZ7DpQxtMzxjMkPcXtkFoslAUwF8hV1a+c5y/jLYjGBER1jYdb5y9jd3E5T14zjvS08ByA15hIU1Xj4Zbnl7Eq9wCPXDmW8Rkd3Q4pIEJWAFV1N5AjIrVXIZ8FrA3V9k30+8PidXy+eT+/v2QkY/t0cDucgBOR7SKySkSWi0i2n/UiIv8Qkc0islJE7AumaTGPR/nlyyv5dGMBf7xkFOcM7+Z2SAET6q47Pwaed3qAbgWuDfH2TZR6MTuHZz7fzrUnZ/CDzN5uhxNMZ6rqvnrWTQYGObeJwOPOvTHN9qe31/Ha17v4xbmDmTqhj9vhBFRIC6CqLgcyQ7lNE/2W7iji7tdWc8rAztx1/jC3w3HTFGCuqirwpXPdbXdVzXc7MBOZZn26haf+u43pJ/blR2cOdDucgIu8KxeN8ZFfXMZNzy0lPS2ZR648noQIvBi3CRR4T0SWishMP+t7Ajk+z3OdZcY02StLc/nj4vVccFx3fvPdEVHZmzoyr140Bu/cYzc9t5SyympeuHEi7dtE7vVIjXSyquaJSFfgfRFZr6qf+qz39x9K/b2RU0BnAvTpE12HtUzLfbx+L798ZSUnDejEX38wOmrHz43qr8smeqkqd7yykpW5xTx8+RgGd4v8LtkNUdU8534v8BowoU6TXMD3BGgvIK+e97JLjoxfy3YW8cPnlzE0PYUnrxlHUkL0XkpkBdBEpKf+u5XXl+fx83MGc+6IdLfDCToRaSsiKbWPgXOB1XWaLQKmOb1BTwCK7fyfaYrNew9x3bNZdE1N4tlrJ5AS5YPH2yFQE3E+2bCXB95ez/mj0rn1O9F3Yr4e3YDXnPMwCcALqvqOiNwMoKpPAIuB84HNQCnWy9o0we7icqbPXkJCnDD3ugl0SUlyO6SgswJoIsqWgkP8eP7XDElP5aHLRkfliXl/VHUrMNrP8id8Hivwo1DGZaJDcWkV02cvobisigUzT6Bvp7YNvygK2CFQEzFKyqu4cW42reLjeGrauIgdgd6YcFJeVcMNc7PYuu8Qs64Zx8ieaW6HFDL2H8REhBqP8pP5X7NzfynzbphIrw42k5YxLVVd4+HWF74me0cR/7zieE4a2NntkELK9gBNRPi/dzfw8YYC7r1oBCf07+R2OMZEPFXl7tdX88G6Pdx74XAuPK6H2yGFnBVAE/beWL6LJ/6zhSsn9uGaE/q6HY4xUeGv729kQVYOt545kBkn93M7HFdYATRhbVVuMb98eSUTMjry2++OcDscY6LCnP9t558fbebyzN78/NzBbofjGiuAJmztPVjOzOey6dwuiceuHktigv25GtNSb67M47f/XsPZw7rxh0tGxkxPan9C/h9FROKdGeHfDPW2TeSoqK7hlnnLKCqtZNa0cXRuF/3XJBkTbP/bvI/bF64gs2+HWBg7t0Fu/PQ/Ada5sF0TIVSV37y+hqU7injostGM6BE73bKNCZbVu4qZ+dxSMjq34V/TxpPcKnqHOGuskBZAEekFXAD8K5TbNZFl7hc7WJjtPTkfiz3TjAm0HfsPM+OZLNJat2LudRNJaxPdQ5w1Vqj3AP8G/BLw+FspIjNFJFtEsgsKCkIbmQkL/9u8j/veXMvZw7py+zmxe3LemEApOFjBtNlLqPZ4mHPdBNLTkt0OKWyErACKyIXAXlVdWl8bG6E+tuUUlvLDF5bRv3NbHr58DHFROgWLMaFysLyKGc8sYU9JObNnjGdg13ZuhxRWQrkHeDJwkYhsBxYA3xGReSHcvgljhyuquXFuNqrw1LTMqB+F3phgq6iu4eZ5S1m/+yCPXzWOsX06uB1S2AlZAVTVO1W1l6pmAFOBj1T16lBt34Qvj0e5/cXlbNxzkEeuPJ6MzrExEK8xweLNqRV8vnk/D156HGcO7ep2SGEptvvAmrDwj4828e6aPfz6/GGcOsgOfRvTEqrK7/69hrdW5nPH5KFcOq6X2yGFLVcGw1bVT4BP3Ni2CS/vrM7nbx9s4tKxvbj+lNgcjsmYQHrsky3M+WIHN5zSj5tO6+92OGHN9gCNa9bvLuH2F1cwpnf7mB+RoiEi0ltEPhaRdSKyRkR+4qfNGSJSLCLLndtv3IjVuGdh1k7+790NXDymB78+f5jlVANsOiTjisLDldwwJ5t2SQk8ec04uyi3YdXAz1V1mYikAEtF5H1VXVun3X9V9UIX4jMue3/tHu58dRWnDe7Cg98fbb2oG8H2AE3IVdV4+NHzy9h7sIJZ0zLplmrXJTVEVfNVdZnz+CDe0ZR6uhuVCRfZ2wu59YVljOqZxuNX2bi5jWWfkgm537+5li+27ueB741iTO/2bocTcUQkAzge+MrP6hNFZIWIvC0i9U6fYYNORI+New5y3bNZ9GjfmtkzxtM2yQ7sNZYVQBNSC7N2MueLHdx4aj++N9Z6pzWViLQDXgF+qqoldVYvA/qq6mjgn8Dr9b2PDToRHXYdKGPa00tIbhXP3Osm0MkGjW8SK4AmZLK3F3L366s5dVBnfjVpqNvhRBwRaYW3+D2vqq/WXa+qJap6yHm8GGglIp1DHKYJkaLDlUyfvYTDFdXMuW4CvTu2cTukiGMF0IRE3oEybp63lF4d2vDIFWNjfhqWphJvd76ngXWq+td62qQ77RCRCXjze3/oojShUlpZzXVzsthZWMpT0zMZ1j3V7ZAikh0sNkFXVlnDzOeyKa/ysGDmOBuJvnlOBq4BVonIcmfZr4E+AKr6BPB94BYRqQbKgKmqqm4Ea4KnqsbDrS98zYqcAzx21VhO6N/J7ZAilhVAE1Sqyq9eWcmavBL+NS2TgV1T3A4pIqnqZ8Ax+7Wr6iPAI6GJyLhBVbnjlVV8tH4vf7hkJJNGdnc7pIhmx6FMUD3xn60sWpHHL84dwlnDurkdjjER7c/vbOCVZbn87OzBXDWxr9vhRDwrgCZoPlq/hwffXc93R/fgh2cMcDscYyLa059t44n/bOHqE/pw21kD3Q4nKlgBNEGxee8hfjJ/OSN6pPLgpcfZkEzGtMAby3dx/5trmTwynd9dZMMGBkooJ8RtcCxDEx2Ky6qYOTebpFZxPHlNJq0TbZgzY5rr040F/OKlFUzs15GHLx9DvA1xFjCh7ATT2LEMTQSr8Si3zf+anKJSXrjxBHq2b+12SMZErBU5B7h53lIGdk3hqemZNmZugIVyQlwbyzAGPPjOev6zsYD7poxkfEZHt8MxJmJtLTjEtc9m0bFtInOuHU9qsl0+FGiunAOsbyxDG58wsr32dS5PfrqVaSf25YoJfdwOx5iItbeknGmzlyDAc9dPpKsNGB8UIS+AxxrL0MYnjFwrcg7wq1dWcUL/jtxz4XC3wzEmYpWUVzH9mSwKD1fyzLXj6de5rdshRa2QFsCGxjI0kWlvSTkzn8uma0oSj101jlY2zJkxzVJeVcONc7LZvPcgT1w9juN62WwpwRSyTjCNGcvQRJ6K6hpunreUg+XVvHLLSXRsm+h2SMZEpBqP8tMFy/lqWyF/nzqG0wbbUbBgC+VX9dqxDL8jIsud2/kh3L4JMFXl7tdWs2znAf5y2WgbkNeYZlJVfvPGat5Zs5t7LhzOlDHWPzAUQrYH2JixDE1kefZ/23lpaS63nTWIyaNsTEJjmusfH27m+a92cvPpA7j+lH5uhxMz7GSNaZbPN+/j92+t47wR3fjpWYPcDseYiPX8Vzt4+IONXDq2F7+aNMTtcGKKFUDTZDv2H+aHzy9jYJd2/OUHY4izkSmMaZZ3Vudzz+ur+c7Qrjxw6Sgb4izErACaJjlUUc2Nc7MRgaemZdIuyWbUChURmSQiG0Rks4jc4Wd9kogsdNZ/5Vxva8LUl1v3c9uC5Yzu3Z5HrxxrvaddYJ+4aTSPR/nZwuVsKTjMY1eOpU+nNm6HFDNEJB54FJgMDAeuEJG6F1xeDxSp6kDgYeDPoY3SNNbavBJunJNNn45tmD19vI2X6xIrgKbR/vbBRt5fu4d7LhjGSQM7ux1OrJkAbFbVrapaCSwAptRpMwWY4zx+GThL7Jha2Nm5v5TpzyyhbVICc66bQAe7dMg1dvzKNMpbK/P5x0eb+UFmL6aflOF2OLGoJ5Dj8zwXmFhfG1WtFpFioBOwLyQRGsoqa8gvLmN3cTn5xeXkF5eRX1zO7uJy8orL2V1cRlFpFanJCbx8y0k2WLzLrACaBq3NK+EXL61gXN8O3H+xzUXmEn8fujajjbehyExgJkCfPjZua2OUVlaTd6DcKW5lRxW1/OJydpeUc6C06luv69g2kfTUZHq2T2Zc3/Z0T2vNucO7Mahbigs/hfFlBdAc0/5DFdw4N5v2bVrx+NVjSUqwcxUuyQV6+zzvBeTV0yZXRBKANKDQ35up6ixgFkBmZqbfIhlLDlVUHylk+QfKnYJWdlTBKymv/tbrOrdLJD0tmV4d2jA+oyPd2yeTnppM97TWdE9LJj0t2aYwCmNWAE29qmo8/PD5Zew7VMFLN59I1xQbkd5FWcAgEekH7AKmAlfWabMImA58AXwf+EhVY764lZRXHTkkubvYp6iVOHtvB8o5WOGvuCXRPS2ZPp3acEL/jqQ7Rc17a03X1CQrbhHOCqCp1+/+vebIuIQ2KK+7nHN6twLvAvHAbFVdIyL3AdmqugjvWLvPichmvHt+U92LOPhUlZLy6qPOs3n34MrYXVJ+ZNmhOsVN5Jvi1q9zW04a0Jl0p7ClpybTo723uNnRjuhnBdD49fxXO5j35U5uOr2/jUsYJlR1MbC4zrLf+DwuBy4LdVzBoKoUl1X5dCAp89mL++Z5aWXNUa8Tga4pSaSntWZgl3acMrAzPdonk57W2jk0mUy31GQSE6wDvAlxARSRScDf8X6D/ZeqPhDK7ZvGWbKtkHvfWMMZQ7rwy/OGuh2OiTKqSlFpVb29JWuXlVUdXdziBLqmJNO9fTJD01M4Y3BX7+HI9t8Utm6pyXZBuWm0UE6HVHsh7zl4T9ZnicgiVV0bqhhMw3KLSrll3lL6dGrD36ceT7wNc2aaQFUpPFzpFLVvekjm+/SczC8up6Lac9Tr4uOEbilJpKclM6x7Kt8Z2tU5LNn6SMeSrilJJFhxMwEUyj3AIxfyAohI7YW8VgDDRGllNTPnLqWyxsNT0zJJa93K7ZBMGPF4lP2HK4/0ivRX5HaXlFNZp7glxAndnMOPI3umcc7wbkf1kuye1pouKUn2ZcuEXCgLYGMu5G2UnftLefDd9QEJynxjZ2Ep63aXMHvGeAZ0aed2OMZlBQcruP/NtU6PyTL2FFdQWXN0cWsVL3RNSaZH+2TG9G5/VFGr7THZqZ0VNxOeQlkAG7xIt7EX55ZV1bA2vySgwRnvL+j3F4/kzCFd3Q7FhIHEhDiW5xwgPS2ZsX061Nlr8xa5Tm0TbTYQE7FCWQAbvJC3sRfnDklP4aOfnxGEEI0xtdJat+LTX57pdhjGBE0ozygfuZBXRBLxXqO0KITbN8YYY44I2R5gfRfyhmr7xhhjjK+QXgfo70JeY4wxxg0SrkMFikgBsKOBZp2JjKleIiVOiJxYIyVOgCGqGrZD/zci1yLls46UOMFiDYYm51nYDoWmql0aaiMi2aqaGYp4WiJS4oTIiTVS4gRvrG7HcCwN5VqkfNaREidYrMHQnDyzYRWMMcbEJCuAxhhjYlKkF8BZbgfQSJESJ0ROrJESJ0RWrP5ESvyREidYrMHQ5DjDthOMMcYYE0yRvgdojDHGNEvEFUARuUxE1oiIR0Qy66y7U0Q2i8gGETnPrRj9EZHfisguEVnu3M53OyZfIjLJ+dw2i8gdbsdzLCKyXURWOZ9jWPWwFJHZIrJXRFb7LOsoIu+LyCbnvoObMTaG5VlwWJ4FRqDyLOIKILAa+B7wqe9CERmOd3i1EcAk4DFnDsJw8rCqjnFuYTMggM9cjZOB4cAVzucZzs50Psdw6579LN6/P193AB+q6iDgQ+d5uLM8CzDLs4B6lgDkWcQVQFVdp6ob/KyaAixQ1QpV3QZsxjsHoWnYkbkaVbUSqJ2r0TSRqn4KFNZZPAWY4zyeA1wc0qCawfIsKCzPAiRQeRZxBfAY/M032NOlWOpzq4isdHbfw+kwWCR8dr4UeE9EljpTaIW7bqqaD+DcR/J8U5Hwt2J5FhhRn2dhORKMiHwApPtZdZeqvlHfy/wsC2kX12PFDTwO3O/EdD/wF+C60EV3TK5/dk10sqrmiUhX4H0RWe98IzRNYHkWcq5/dk0U9XkWlgVQVc9uxssanG8w2Bobt4g8BbwZ5HCawvXPrilUNc+53ysir+E9tBTOiblHRLqrar6IdAf2uh0QWJ65wPXPriliIc+i6RDoImCqiCSJSD9gELDE5ZiOcH4htS7B28kgXETMXI0i0lZEUmofA+cSXp+lP4uA6c7j6UB9e1eRwPKs+SzPgqvpeaaqEXXD+0edC1QAe4B3fdbdBWwBNgCT3Y61TtzPAauAlc4vqrvbMdWJ73xgo/P53eV2PMeIsz+wwrmtCbdYgflAPlDl/J1eD3TC2yttk3Pf0e04G/FzWJ4FJz7Ls8DEF5A8s5FgjDHGxKRoOgRqjDHGNJoVQGOMMTHJCqAxxpiYZAXQGGNMTLICaIwxJiZZATTGGBOTrAAaY4yJSVYAjTHGxKT/D/lYLvvPoJ81AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x =  [i for i in range(-10,10)]\n",
    "fn_tanh = lambda x : ( math.exp(x) - math.exp(-x))/( math.exp(x) + math.exp(-x))\n",
    "fn_sigmoid = lambda x : 1/(1+ math.exp(-x))\n",
    "fn_relu = lambda x : max(0,x)\n",
    "fn_leaky_relu = lambda x : max(0.1*x,x)\n",
    "fig, axs =plt.subplots(nrows=2, ncols=2,constrained_layout=True)\n",
    "axs[0][0].plot(x, list(map(fn_tanh,x)))\n",
    "axs[0][0].set_title(\"Tanh\")\n",
    "axs[0][1].plot(x, list(map(fn_sigmoid,x)))\n",
    "axs[0][1].set_title(\"Sigmoid\")\n",
    "axs[1][0].plot(x, list(map(fn_relu,x)))\n",
    "axs[1][0].set_title(\"Relu\")\n",
    "axs[1][1].plot(x, list(map(fn_leaky_relu,x)))\n",
    "axs[1][1].set_title(\"leaky relu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n",
       "  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from ipywidgets import interact\n",
    "from bokeh.models import CustomJS, Slider\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.io import show, output_file, output_notebook, push_notebook\n",
    "from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes\n",
    "from bokeh.palettes import Spectral4\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(G, g_layout=nx.spring_layout ):\n",
    "    \n",
    "    plot = Plot(plot_width=400, plot_height=400,\n",
    "                x_range=Range1d(-1.1,1.1),\n",
    "                y_range=Range1d(-1.1,1.1))\n",
    "    plot.title.text = \"Neural Net, \"\n",
    "\n",
    "    plot.add_tools(HoverTool(tooltips=[(\"index\",\"@index\"),(\"weight\",\"@weight\")]),\n",
    "                   TapTool(), BoxSelectTool())\n",
    "\n",
    "    graph_renderer = from_networkx(G, g_layout, scale=1, center=(0,0))\n",
    "    \n",
    "    graph_renderer.node_renderer.data_source.add(\n",
    "                list(dict(G.nodes(data='weight')).values()), 'weight')\n",
    "    graph_renderer.node_renderer.data_source.add(list(G.nodes()), 'index')\n",
    "\n",
    "    graph_renderer.node_renderer.glyph = Circle(size=30, fill_color=Spectral4[0])\n",
    "    graph_renderer.node_renderer.selection_glyph = Circle(size=30, fill_color=Spectral4[2])\n",
    "    graph_renderer.node_renderer.hover_glyph = Circle(size=30, fill_color=Spectral4[1])\n",
    "\n",
    "    graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width=5)\n",
    "    graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=5)\n",
    "    graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=5)\n",
    "    #graph_renderer.edge_renderer.\n",
    "    graph_renderer.edge_renderer.data_source.add(list(G.edges()), 'index')\n",
    "\n",
    "    graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "    #graph_renderer.inspection_policy = EdgesAndLinkedNodes()\n",
    "\n",
    "    plot.renderers.append(graph_renderer)\n",
    "\n",
    "    return plot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproximar cualquier funcion.\n",
    "Si tenemos una funcion $f(x)= -x $ como la representamos con los valores de **w_0_1** y **b_1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"060130a7-c0c6-413d-9699-83c64c7fa62f\" data-root-id=\"1070\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"d36f22ab-8270-48f0-9ab1-372569017552\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1004\",\"type\":\"Plot\"}]},\"id\":\"1070\",\"type\":\"Row\"},{\"attributes\":{\"source\":{\"id\":\"1018\",\"type\":\"ColumnDataSource\"}},\"id\":\"1020\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_color\":{\"value\":\"#fdae61\"},\"line_width\":{\"value\":5}},\"id\":\"1054\",\"type\":\"MultiLine\"},{\"attributes\":{\"callback\":null,\"data\":{\"end\":[\"a_1\"],\"index\":[[\"a_0\",\"a_1\"]],\"start\":[\"a_0\"],\"weight\":[0.5]},\"selected\":{\"id\":\"1087\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1088\",\"type\":\"UnionRenderers\"}},\"id\":\"1022\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1022\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1049\",\"type\":\"MultiLine\"},\"hover_glyph\":{\"id\":\"1059\",\"type\":\"MultiLine\"},\"muted_glyph\":null,\"selection_glyph\":{\"id\":\"1054\",\"type\":\"MultiLine\"},\"view\":{\"id\":\"1024\",\"type\":\"CDSView\"}},\"id\":\"1023\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"1022\",\"type\":\"ColumnDataSource\"}},\"id\":\"1024\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"end\":1.1,\"start\":-1.1},\"id\":\"1002\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"NodesAndLinkedEdges\"},{\"attributes\":{\"callback\":null,\"end\":1.1,\"start\":-1.1},\"id\":\"1003\",\"type\":\"Range1d\"},{\"attributes\":{\"graph_layout\":{\"a_0\":[1.0,4.3711388286737916e-08],\"a_1\":[-1.0,-4.3711388286737916e-08]}},\"id\":\"1025\",\"type\":\"StaticLayoutProvider\"},{\"attributes\":{\"fill_color\":{\"value\":\"#2b83ba\"},\"size\":{\"units\":\"screen\",\"value\":30}},\"id\":\"1034\",\"type\":\"Circle\"},{\"attributes\":{\"plot_height\":400,\"plot_width\":400,\"renderers\":[{\"id\":\"1016\",\"type\":\"GraphRenderer\"}],\"title\":{\"id\":\"1007\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1011\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1002\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"1073\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1003\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"1072\",\"type\":\"LinearScale\"}},\"id\":\"1004\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"LinearScale\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1084\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"text\":\"Neural Net, \"},\"id\":\"1007\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1087\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"index\",\"@index\"],[\"weight\",\"@weight\"]]},\"id\":\"1008\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1009\",\"type\":\"TapTool\"},{\"attributes\":{\"callback\":null,\"overlay\":{\"id\":\"1084\",\"type\":\"BoxAnnotation\"}},\"id\":\"1010\",\"type\":\"BoxSelectTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#abdda4\"},\"size\":{\"units\":\"screen\",\"value\":30}},\"id\":\"1044\",\"type\":\"Circle\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1008\",\"type\":\"HoverTool\"},{\"id\":\"1009\",\"type\":\"TapTool\"},{\"id\":\"1010\",\"type\":\"BoxSelectTool\"}]},\"id\":\"1011\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"NodesOnly\"},{\"attributes\":{},\"id\":\"1086\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"edge_renderer\":{\"id\":\"1023\",\"type\":\"GlyphRenderer\"},\"inspection_policy\":{\"id\":\"1075\",\"type\":\"NodesOnly\"},\"layout_provider\":{\"id\":\"1025\",\"type\":\"StaticLayoutProvider\"},\"node_renderer\":{\"id\":\"1019\",\"type\":\"GlyphRenderer\"},\"selection_policy\":{\"id\":\"1068\",\"type\":\"NodesAndLinkedEdges\"}},\"id\":\"1016\",\"type\":\"GraphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"#fdae61\"},\"size\":{\"units\":\"screen\",\"value\":30}},\"id\":\"1039\",\"type\":\"Circle\"},{\"attributes\":{\"line_alpha\":{\"value\":0.8},\"line_color\":{\"value\":\"#CCCCCC\"},\"line_width\":{\"value\":5}},\"id\":\"1049\",\"type\":\"MultiLine\"},{\"attributes\":{},\"id\":\"1085\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"data\":{\"index\":[\"a_0\",\"a_1\"],\"weight\":[1.0,0.3775406687981454]},\"selected\":{\"id\":\"1085\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1086\",\"type\":\"UnionRenderers\"}},\"id\":\"1018\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":{\"value\":\"#abdda4\"},\"line_width\":{\"value\":5}},\"id\":\"1059\",\"type\":\"MultiLine\"},{\"attributes\":{\"data_source\":{\"id\":\"1018\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1034\",\"type\":\"Circle\"},\"hover_glyph\":{\"id\":\"1044\",\"type\":\"Circle\"},\"muted_glyph\":null,\"selection_glyph\":{\"id\":\"1039\",\"type\":\"Circle\"},\"view\":{\"id\":\"1020\",\"type\":\"CDSView\"}},\"id\":\"1019\",\"type\":\"GlyphRenderer\"}],\"root_ids\":[\"1070\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"d36f22ab-8270-48f0-9ab1-372569017552\",\"notebook_comms_target\":\"1089\",\"roots\":{\"1070\":\"060130a7-c0c6-413d-9699-83c64c7fa62f\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1070"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x =  1.0\n",
    "w_0_1 = 0.5\n",
    "b_1 = -1.0\n",
    "pl = None \n",
    "\n",
    "G=nx.DiGraph()\n",
    "G.add_node(\"a_0\",weight=x)\n",
    "G.add_node(\"a_1\",weight=fn_sigmoid(w_0_1*x + b_1))\n",
    "G.add_edge(\"a_0\",\"a_1\", weight=w_0_1)\n",
    "\n",
    "pl = plot_network(G, nx.kamada_kawai_layout)\n",
    "p_row = row(pl) \n",
    "bokeh_graph = show(p_row, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979a4825a41340eca5117506e83f1ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='f', options=('fn_sigmoid', 'fn_tanh', 'fn_relu', 'fn_leaky_relu'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update(f, x, w_0_1, b_1)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update(f,x, w_0_1, b_1):\n",
    "    fn_act = eval(f) \n",
    "    G.add_node(\"a_0\",weight=x)\n",
    "    G.add_node(\"a_1\",weight=fn_act(w_0_1*x + b_1))\n",
    "    G.add_edge(\"a_0\",\"a_1\", weight=w_0_1)\n",
    "    pl = plot_network(G,g_layout=nx.shell_layout)\n",
    "    p_row.children=[pl]\n",
    "    push_notebook(handle=bokeh_graph)\n",
    "\n",
    "interact(update,f=[\"fn_sigmoid\",\"fn_tanh\",\"fn_relu\",\"fn_leaky_relu\"],\n",
    "         x=(-1,1,0.1),w_0_1=(-10,10,0.1),b_1=(-10,10,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora practiquemos con una funcion mas compleja.\n",
    "\n",
    "Pinta una linea para separar los siguientes datos\n",
    "\n",
    " 0   1\n",
    "\n",
    " 1   0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"6300ce9a-b3c3-4bb2-a372-af5a24312cd1\" data-root-id=\"68522\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"a36709c5-cb6c-41e2-b0ac-57f56d9ec809\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"68452\",\"type\":\"Plot\"}]},\"id\":\"68522\",\"type\":\"Row\"},{\"attributes\":{\"fill_color\":{\"value\":\"#fdae61\"},\"size\":{\"units\":\"screen\",\"value\":30}},\"id\":\"68491\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"68648\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"data\":{\"index\":[\"a_0\",\"a_1\",\"a_2\",\"a_3\"],\"weight\":[1.0,0.3775406687981454,0.3775406687981454,0.3492223217852996]},\"selected\":{\"id\":\"68645\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"68646\",\"type\":\"UnionRenderers\"}},\"id\":\"68466\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"68632\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"68520\",\"type\":\"NodesAndLinkedEdges\"},{\"attributes\":{\"line_alpha\":{\"value\":0.8},\"line_color\":{\"value\":\"#CCCCCC\"},\"line_width\":{\"value\":5}},\"id\":\"68501\",\"type\":\"MultiLine\"},{\"attributes\":{\"source\":{\"id\":\"68466\",\"type\":\"ColumnDataSource\"}},\"id\":\"68468\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null},\"id\":\"68457\",\"type\":\"TapTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#abdda4\"},\"size\":{\"units\":\"screen\",\"value\":30}},\"id\":\"68496\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"68466\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"68486\",\"type\":\"Circle\"},\"hover_glyph\":{\"id\":\"68496\",\"type\":\"Circle\"},\"muted_glyph\":null,\"selection_glyph\":{\"id\":\"68491\",\"type\":\"Circle\"},\"view\":{\"id\":\"68468\",\"type\":\"CDSView\"}},\"id\":\"68467\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"68646\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"end\":1.1,\"start\":-1.1},\"id\":\"68450\",\"type\":\"Range1d\"},{\"attributes\":{\"line_color\":{\"value\":\"#abdda4\"},\"line_width\":{\"value\":5}},\"id\":\"68511\",\"type\":\"MultiLine\"},{\"attributes\":{\"text\":\"Neural Net, \"},\"id\":\"68455\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"index\",\"@index\"],[\"weight\",\"@weight\"]]},\"id\":\"68456\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"68647\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"68635\",\"type\":\"NodesOnly\"},{\"attributes\":{\"fill_color\":{\"value\":\"#2b83ba\"},\"size\":{\"units\":\"screen\",\"value\":30}},\"id\":\"68486\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"68633\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"data\":{\"end\":[\"a_1\",\"a_2\",\"a_3\",\"a_3\"],\"index\":[[\"a_0\",\"a_1\"],[\"a_0\",\"a_2\"],[\"a_1\",\"a_3\"],[\"a_2\",\"a_3\"]],\"start\":[\"a_0\",\"a_0\",\"a_1\",\"a_2\"],\"weight\":[0.5,0.5,0.5,0.5]},\"selected\":{\"id\":\"68647\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"68648\",\"type\":\"UnionRenderers\"}},\"id\":\"68470\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"68645\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"68470\",\"type\":\"ColumnDataSource\"}},\"id\":\"68472\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"68470\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"68501\",\"type\":\"MultiLine\"},\"hover_glyph\":{\"id\":\"68511\",\"type\":\"MultiLine\"},\"muted_glyph\":null,\"selection_glyph\":{\"id\":\"68506\",\"type\":\"MultiLine\"},\"view\":{\"id\":\"68472\",\"type\":\"CDSView\"}},\"id\":\"68471\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot_height\":400,\"plot_width\":400,\"renderers\":[{\"id\":\"68464\",\"type\":\"GraphRenderer\"}],\"title\":{\"id\":\"68455\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"68459\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"68450\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"68633\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"68451\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"68632\",\"type\":\"LinearScale\"}},\"id\":\"68452\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"overlay\":{\"id\":\"68644\",\"type\":\"BoxAnnotation\"}},\"id\":\"68458\",\"type\":\"BoxSelectTool\"},{\"attributes\":{\"graph_layout\":{\"a_0\":[0.7255765670057706,-0.957755723730932],\"a_1\":[1.0,0.7201305535541147],\"a_2\":[-0.9837455138627698,-0.7385148595732032],\"a_3\":[-0.7418310531430011,0.97614002975002]}},\"id\":\"68477\",\"type\":\"StaticLayoutProvider\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"68644\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"edge_renderer\":{\"id\":\"68471\",\"type\":\"GlyphRenderer\"},\"inspection_policy\":{\"id\":\"68635\",\"type\":\"NodesOnly\"},\"layout_provider\":{\"id\":\"68477\",\"type\":\"StaticLayoutProvider\"},\"node_renderer\":{\"id\":\"68467\",\"type\":\"GlyphRenderer\"},\"selection_policy\":{\"id\":\"68520\",\"type\":\"NodesAndLinkedEdges\"}},\"id\":\"68464\",\"type\":\"GraphRenderer\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"68456\",\"type\":\"HoverTool\"},{\"id\":\"68457\",\"type\":\"TapTool\"},{\"id\":\"68458\",\"type\":\"BoxSelectTool\"}]},\"id\":\"68459\",\"type\":\"Toolbar\"},{\"attributes\":{\"line_color\":{\"value\":\"#fdae61\"},\"line_width\":{\"value\":5}},\"id\":\"68506\",\"type\":\"MultiLine\"},{\"attributes\":{\"callback\":null,\"end\":1.1,\"start\":-1.1},\"id\":\"68451\",\"type\":\"Range1d\"}],\"root_ids\":[\"68522\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"a36709c5-cb6c-41e2-b0ac-57f56d9ec809\",\"notebook_comms_target\":\"68649\",\"roots\":{\"68522\":\"6300ce9a-b3c3-4bb2-a372-af5a24312cd1\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "68522"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x =  1.0\n",
    "w_0_1 = 0.5\n",
    "w_0_2 = 0.5\n",
    "w_1_3 = 0.5\n",
    "w_2_3 = 0.5\n",
    "b_1 = -1.0\n",
    "b_2 = -1.0\n",
    "b_3 = -1.0\n",
    "\n",
    "pl2 = None \n",
    "\n",
    "G=nx.DiGraph()\n",
    "G.add_node(\"a_0\",weight=x)\n",
    "G.add_node(\"a_1\",weight=fn_sigmoid(w_0_1*G.nodes[\"a_0\"][\"weight\"] + b_1))\n",
    "G.add_node(\"a_2\",weight=fn_sigmoid(w_0_2*G.nodes[\"a_0\"][\"weight\"] + b_2))\n",
    "G.add_node(\"a_3\",weight=fn_sigmoid(w_2_3*G.nodes[\"a_2\"][\"weight\"]+\n",
    "                                   w_1_3*G.nodes[\"a_1\"][\"weight\"]+ b_3))\n",
    "G.add_edge(\"a_0\",\"a_1\",weight=w_0_1)\n",
    "G.add_edge(\"a_0\",\"a_2\",weight=w_0_2)\n",
    "G.add_edge(\"a_2\",\"a_3\",weight=w_2_3)\n",
    "G.add_edge(\"a_1\",\"a_3\",weight=w_1_3)\n",
    "\n",
    "pl2 = plot_network(G,nx.layout.fruchterman_reingold_layout(G,seed=2))\n",
    "p_row2 = row(pl2) \n",
    "bokeh_graph2 = show(p_row2, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1cdc838cdd42deae52728b24a0f7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='f', options=('fn_sigmoid', 'fn_tanh', 'fn_relu', 'fn_leaky_relu'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update2(f, x, w_0_1, w_0_2, w_1_3, w_2_3, b_1, b_2, b_3)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update2(f,x, w_0_1, w_0_2, w_1_3, w_2_3, b_1, b_2 ,b_3):\n",
    "    fn_act = eval(f) \n",
    "    G.add_node(\"a_0\",weight=x)\n",
    "    G.add_node(\"a_1\",weight=fn_sigmoid(w_0_1*G.nodes[\"a_0\"][\"weight\"] + b_1))\n",
    "    G.add_node(\"a_2\",weight=fn_sigmoid(w_0_2*G.nodes[\"a_0\"][\"weight\"] + b_2))\n",
    "    G.add_node(\"a_3\",weight=fn_sigmoid(w_2_3*G.nodes[\"a_2\"][\"weight\"]+\n",
    "                                       w_1_3*G.nodes[\"a_1\"][\"weight\"]+ b_3))\n",
    "    G.add_edge(\"a_0\",\"a_1\",weight=w_0_1)\n",
    "    G.add_edge(\"a_0\",\"a_2\",weight=w_0_2)\n",
    "    G.add_edge(\"a_2\",\"a_3\",weight=w_2_3)\n",
    "    G.add_edge(\"a_1\",\"a_3\",weight=w_1_3)\n",
    "\n",
    "    pl2 = plot_network(G,nx.layout.fruchterman_reingold_layout(G,seed=2))\n",
    "    p_row2.children=[pl2]\n",
    "    push_notebook(handle=bokeh_graph2)\n",
    "\n",
    "interact(update2,f=[\"fn_sigmoid\",\"fn_tanh\",\"fn_relu\",\"fn_leaky_relu\"],\n",
    "         x=(-1,1,0.1),w_0_1=(-10,10,0.1),w_0_2=(-10,10,0.1),\n",
    "         w_1_3=(-10,10,0.1),w_2_3=(-10,10,0.1),\n",
    "         b_1=(-10,10,0.1),b_2=(-10,10,0.1),b_3=(-10,10,0.1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando Xor con back propagation\n",
    "\n",
    "Para crear la compuerta tenemos los siguiente:\n",
    "Si x > 0.51  entonces las neuronas van a_1 tomaran el valor de 1 y a_2 el valor de 0 y Y el valor de y es 1\n",
    "\n",
    "X|not X|Y|\n",
    "--| -- | --|\n",
    "0|0|0\n",
    "0|1|1\n",
    "1|0|1\n",
    "1|1|0\n",
    "\n",
    "Para construir esta compuerta tenemos datos.\n",
    "X = 0.11 y= 1\n",
    "X = 0.54 y= 1\n",
    "X = 0.533 y = 0\n",
    "\n",
    "Iniamos con los valores de w_0_1, w_0_2, w_1_3, w_2_3 = 0.5 y x=1 y los  b_1, b_2, b_3=0\n",
    "\n",
    "\n",
    "\\$a^{(1)} = \\sigma(W^{(0,1)}.a^{(0)} + b^{(1)})\\$\n",
    "\\$ = \\sigma(0.5 \\* 1. +0)= 0.378 \\$\n",
    "\n",
    "\\$a^{(2)} = \\sigma(W^{(0,2)}.a^{(0)} + b^{(2)})\\$\n",
    "\\$ = \\sigma(0.5 \\* 1 +0)= 0.378 \\$\n",
    "\n",
    "\\$a^{(3)} = \\sigma(W^{(1,3)}.a^{(1)} +W^{(2,3)}.a^{(2)} + b^{(3)}) \\$\n",
    "\n",
    "\\$ = \\sigma(0.5 \\* 0.378 + 0.5 \\* 0.378 + 0) = 0.349 \\$\n",
    "\n",
    "\\$ error = (0.349 - 1.0)^2 = 0.4238\\$\n",
    "\n",
    "Ahora calculemos que tanto debe cambiar $w_{0,2}$ con respecto al error.\n",
    "\n",
    "\n",
    "\\$ w_{0,2} = w_{0,2} - \\delta \\frac{\\partial(error)}{\\partial(w_{0,2})} \\$\n",
    "\n",
    "\\$ \\partial(\\sigma(X)) = \\partial(\\frac{1}{1+e^{-X}})= f(x)(1-f(x)) \\$ https://en.wikipedia.org/wiki/Logistic_function#Derivative\n",
    "\n",
    "\\$ \\frac{\\partial(error)}{\\partial(w_{0,2})} = \\frac{\\partial(error)}{\\partial(a^{(3)})}  \\* \\frac{\\partial(a^{(3)})}{\\partial(a^{(2)})} \\*  \\frac{\\partial(a^{(2)})}{\\partial(w_{0,2})}\\$\n",
    "\n",
    "\\$ \\frac{\\partial(error)}{\\partial(w_{0,2})} = \\frac{\\partial((a_3-y)^2)}{\\partial(a^{(3)})}  \\* \\frac{\\partial(\\sigma(w_{1,3}\\*a_1 + w_{2,3}\\*a_2))}{\\partial(a^{(2)})} \\*  \\frac{\\partial(\\sigma(w_{(0,2)}.X + b^{(2)}))}{\\partial(w_{0,2})}\\$\n",
    "\n",
    "\\$ \\frac{\\partial(error)}{\\partial(w_{0,2})} = 2(a_3-y) \\* \\sigma(w_{1,3}\\*a_1 + w_{2,3}\\*a_2)(1-\\sigma(w_{1,3}\\*a_1 + w_{2,3}\\*a_2)) w_{2,3}  \\*  \\sigma(w_{(0,2)}.X + b^{(1)})(1-\\sigma(w_{(0,2)}.X + b^{(1)}))X\\$\n",
    "\n",
    "Utilizando los valores\n",
    "\n",
    "\\$  \\frac{\\partial(error)}{\\partial(w_{0,2})} =  2 (0.349 - 1) \\* 0.349(1-0.349)\\* 0.5 \\* 0.378 \\* 0.622 * 1  = -0.03477\\$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sigmoid(W*X + b)$\n",
    "= 2(sigmoid(W*X + b) - y ) * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5784495238714773 0.40869751492302014\n"
     ]
    }
   ],
   "source": [
    "X_list = list(np.random.uniform(size=(100)))\n",
    "y_list = list(map( lambda m: 1.0 if m<0.5 else 0.0, X_list))\n",
    "data = list(zip(X_list, y_list))\n",
    "train = data[0:80]\n",
    "test = data[80:]\n",
    "epochs = 100\n",
    "lr =0.1\n",
    "w_t = 0.5\n",
    "for j in range(epochs):\n",
    "    error_l = []\n",
    "    for x,y in train:\n",
    "        pre = fn_sigmoid(w_t*x)\n",
    "        error = (pre - y)\n",
    "        error_l.append(abs(error))\n",
    "        w_t -= lr*(error * pre * (1-pre) * x)\n",
    "    #print(\"Error %s\" % (sum(error_l)/len(error_l)))\n",
    "print(w_t,sum(error_l)/len(error_l) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.21611196912220404\n",
      "Error 0.21882551068745007\n",
      "Error 0.22094574875103498\n",
      "Error 0.22258255635482257\n",
      "Error 0.2238347037697724\n",
      "Error 0.22478548982064792\n",
      "Error 0.2255026005332322\n",
      "Error 0.2260397875101333\n",
      "Error 0.22643911183800808\n",
      "Error 0.22673316662562054\n",
      "Error 0.22694704102400984\n",
      "Error 0.2270999589801726\n",
      "Error 0.22720660240984153\n",
      "Error 0.22727815812818192\n",
      "Error 0.22732313538989124\n",
      "Error 0.22734799867872074\n",
      "Error 0.22735765451240075\n",
      "Error 0.2273558243298269\n",
      "Error 0.2273453292363803\n",
      "Error 0.22732830695416606\n",
      "Error 0.22730637684641009\n",
      "Error 0.22728076529142188\n",
      "Error 0.22725240084673462\n",
      "Error 0.2272219864338653\n",
      "Error 0.22719005406476844\n",
      "Error 0.2271570063165358\n",
      "Error 0.22712314775413095\n",
      "Error 0.22708870873219977\n",
      "Error 0.22705386342126319\n",
      "Error 0.2270187434580508\n",
      "Error 0.22698344828122016\n",
      "Error 0.22694805295674395\n",
      "Error 0.22691261410233526\n",
      "Error 0.22687717437249563\n",
      "Error 0.22684176585377197\n",
      "Error 0.22680641263495094\n",
      "Error 0.22677113275263738\n",
      "Error 0.22673593966398675\n",
      "Error 0.2267008433614881\n",
      "Error 0.226665851216783\n",
      "Error 0.22663096861937645\n",
      "Error 0.22659619946008167\n",
      "Error 0.22656154649694155\n",
      "Error 0.2265270116321858\n",
      "Error 0.22649259612185202\n",
      "Error 0.22645830073443357\n",
      "Error 0.2264241258709482\n",
      "Error 0.2263900716558011\n",
      "Error 0.22635613800554685\n",
      "Error 0.2263223246809151\n",
      "Error 0.22628863132617857\n",
      "Error 0.22625505749893024\n",
      "Error 0.22622160269260908\n",
      "Error 0.22618826635353256\n",
      "Error 0.22615504789377341\n",
      "Error 0.226121946700892\n",
      "Error 0.22608896214528648\n",
      "Error 0.22605609358574164\n",
      "Error 0.22602334037361418\n",
      "Error 0.22599070185598577\n",
      "Error 0.2259581773780365\n",
      "Error 0.2259257662848262\n",
      "Error 0.22589346792262832\n",
      "Error 0.2258612816399269\n",
      "Error 0.22582920678815813\n",
      "Error 0.22579724272225626\n",
      "Error 0.22576538880105562\n",
      "Error 0.22573364438758042\n",
      "Error 0.22570200884925268\n",
      "Error 0.22567048155803474\n",
      "Error 0.22563906189052782\n",
      "Error 0.22560774922803123\n",
      "Error 0.22557654295657664\n",
      "Error 0.2255454424669408\n",
      "Error 0.22551444715464464\n",
      "Error 0.22548355641993978\n",
      "Error 0.225452769667787\n",
      "Error 0.22542208630782926\n",
      "Error 0.22539150575435807\n",
      "Error 0.2253610274262785\n",
      "Error 0.22533065074707137\n",
      "Error 0.2253003751447525\n",
      "Error 0.22527020005183393\n",
      "Error 0.22524012490527956\n",
      "Error 0.22521014914646603\n",
      "Error 0.22518027222113815\n",
      "Error 0.225150493579368\n",
      "Error 0.2251208126755131\n",
      "Error 0.22509122896817324\n",
      "Error 0.22506174192015077\n",
      "Error 0.22503235099840696\n",
      "Error 0.22500305567402332\n",
      "Error 0.22497385542215995\n",
      "Error 0.22494474972201567\n",
      "Error 0.22491573805678877\n",
      "Error 0.22488681991363663\n",
      "Error 0.22485799478363852\n",
      "Error 0.2248292621617563\n",
      "Error 0.2248006215467961\n",
      "Error 0.2247720724413717\n",
      "Error 0.22474361435186774\n",
      "Error 0.22471524678840155\n",
      "Error 0.22468696926478932\n",
      "Error 0.22465878129850783\n",
      "Error 0.22463068241066053\n",
      "Error 0.22460267212594243\n",
      "Error 0.22457474997260576\n",
      "Error 0.22454691548242528\n",
      "Error 0.22451916819066486\n",
      "Error 0.22449150763604414\n",
      "Error 0.22446393336070591\n",
      "Error 0.22443644491018264\n",
      "Error 0.22440904183336535\n",
      "Error 0.22438172368247106\n",
      "Error 0.22435449001301136\n",
      "Error 0.22432734038376237\n",
      "Error 0.22430027435673222\n",
      "Error 0.22427329149713252\n",
      "Error 0.22424639137334673\n",
      "Error 0.22421957355690125\n",
      "Error 0.2241928376224366\n",
      "Error 0.2241661831476771\n",
      "Error 0.22413960971340327\n",
      "Error 0.22411311690342312\n",
      "Error 0.22408670430454386\n",
      "Error 0.2240603715065444\n",
      "Error 0.22403411810214782\n",
      "Error 0.22400794368699492\n",
      "Error 0.22398184785961614\n",
      "Error 0.2239558302214062\n",
      "Error 0.2239298903765971\n",
      "Error 0.22390402793223285\n",
      "Error 0.22387824249814345\n",
      "Error 0.22385253368691962\n",
      "Error 0.2238269011138878\n",
      "Error 0.2238013443970855\n",
      "Error 0.22377586315723588\n",
      "Error 0.2237504570177251\n",
      "Error 0.22372512560457714\n",
      "Error 0.22369986854643056\n",
      "Error 0.2236746854745148\n",
      "Error 0.22364957602262675\n",
      "Error 0.22362453982710848\n",
      "Error 0.2235995765268246\n",
      "Error 0.22357468576313835\n",
      "Error 0.22354986717989167\n",
      "Error 0.22352512042338119\n",
      "Error 0.22350044514233774\n",
      "Error 0.22347584098790443\n",
      "Error 0.22345130761361492\n",
      "Error 0.22342684467537363\n",
      "Error 0.22340245183143384\n",
      "Error 0.22337812874237742\n",
      "Error 0.22335387507109447\n",
      "Error 0.22332969048276302\n",
      "Error 0.2233055746448292\n",
      "Error 0.22328152722698805\n",
      "Error 0.22325754790116342\n",
      "Error 0.22323363634148832\n",
      "Error 0.22320979222428647\n",
      "Error 0.2231860152280535\n",
      "Error 0.22316230503343765\n",
      "Error 0.22313866132322166\n",
      "Error 0.22311508378230363\n",
      "Error 0.22309157209768088\n",
      "Error 0.2230681259584295\n",
      "Error 0.22304474505568872\n",
      "Error 0.22302142908264208\n",
      "Error 0.22299817773450034\n",
      "Error 0.22297499070848475\n",
      "Error 0.22295186770380923\n",
      "Error 0.22292880842166385\n",
      "Error 0.22290581256519865\n",
      "Error 0.22288287983950655\n",
      "Error 0.222860009951607\n",
      "Error 0.2228372026104309\n",
      "Error 0.22281445752680223\n",
      "Error 0.22279177441342504\n",
      "Error 0.2227691529848653\n",
      "Error 0.22274659295753718\n",
      "Error 0.22272409404968713\n",
      "Error 0.22270165598137798\n",
      "Error 0.22267927847447525\n",
      "Error 0.22265696125263065\n",
      "Error 0.22263470404126862\n",
      "Error 0.22261250656757117\n",
      "Error 0.22259036856046338\n",
      "Error 0.22256828975059967\n",
      "Error 0.22254626987034848\n",
      "Error 0.22252430865377965\n",
      "Error 0.2225024058366492\n",
      "Error 0.22248056115638692\n",
      "Error 0.22245877435208197\n",
      "Error 0.22243704516446877\n",
      "Error 0.22241537333591493\n",
      "Error 0.22239375861040758\n",
      "Error 0.22237220073354047\n",
      "Error 0.2223506994525001\n",
      "Error 0.22232925451605395\n",
      "Error 0.22230786567453714\n",
      "Error 0.22228653267984058\n",
      "Error 0.22226525528539773\n",
      "Error 0.2222440332461723\n",
      "Error 0.2222228663186471\n",
      "Error 0.2222017542608104\n",
      "Error 0.22218069683214522\n",
      "Error 0.22215969379361708\n",
      "Error 0.22213874490766145\n",
      "Error 0.2221178499381739\n",
      "Error 0.22209700865049692\n",
      "Error 0.2220762208114091\n",
      "Error 0.22205548618911392\n",
      "Error 0.2220348045532287\n",
      "Error 0.22201417567477302\n",
      "Error 0.22199359932615823\n",
      "Error 0.22197307528117666\n",
      "Error 0.22195260331498995\n",
      "Error 0.22193218320411945\n",
      "Error 0.22191181472643504\n",
      "Error 0.22189149766114533\n",
      "Error 0.22187123178878645\n",
      "Error 0.22185101689121228\n",
      "Error 0.2218308527515837\n",
      "Error 0.22181073915435978\n",
      "Error 0.2217906758852864\n",
      "Error 0.22177066273138726\n",
      "Error 0.22175069948095355\n",
      "Error 0.22173078592353468\n",
      "Error 0.22171092184992877\n",
      "Error 0.22169110705217218\n",
      "Error 0.2216713413235313\n",
      "Error 0.22165162445849215\n",
      "Error 0.22163195625275245\n",
      "Error 0.22161233650321038\n",
      "Error 0.22159276500795788\n",
      "Error 0.22157324156627003\n",
      "Error 0.22155376597859658\n",
      "Error 0.221534338046553\n",
      "Error 0.22151495757291192\n",
      "Error 0.22149562436159495\n",
      "Error 0.22147633821766272\n",
      "Error 0.22145709894730725\n",
      "Error 0.22143790635784394\n",
      "Error 0.22141876025770216\n",
      "Error 0.22139966045641774\n",
      "Error 0.22138060676462473\n",
      "Error 0.22136159899404592\n",
      "Error 0.2213426369574878\n",
      "Error 0.22132372046882848\n",
      "Error 0.22130484934301373\n",
      "Error 0.22128602339604603\n",
      "Error 0.22126724244497895\n",
      "Error 0.22124850630790824\n",
      "Error 0.22122981480396464\n",
      "Error 0.22121116775330588\n",
      "Error 0.2211925649771101\n",
      "Error 0.2211740062975674\n",
      "Error 0.22115549153787276\n",
      "Error 0.2211370205222191\n",
      "Error 0.2211185930757899\n",
      "Error 0.22110020902475172\n",
      "Error 0.22108186819624712\n",
      "Error 0.22106357041838845\n",
      "Error 0.22104531552024925\n",
      "Error 0.22102710333185885\n",
      "Error 0.22100893368419486\n",
      "Error 0.2209908064091762\n",
      "Error 0.22097272133965698\n",
      "Error 0.22095467830941873\n",
      "Error 0.22093667715316526\n",
      "Error 0.22091871770651467\n",
      "Error 0.2209007998059936\n",
      "Error 0.2208829232890305\n",
      "Error 0.22086508799394955\n",
      "Error 0.2208472937599642\n",
      "Error 0.2208295404271703\n",
      "Error 0.22081182783654074\n",
      "Error 0.22079415582991885\n",
      "Error 0.22077652425001223\n",
      "Error 0.22075893294038668\n",
      "Error 0.22074138174546026\n",
      "Error 0.2207238705104976\n",
      "Error 0.2207063990816031\n",
      "Error 0.22068896730571574\n",
      "Error 0.22067157503060358\n",
      "Error 0.2206542221048567\n",
      "Error 0.2206369083778837\n",
      "Error 0.2206196336999035\n",
      "Error 0.220602397921941\n",
      "Error 0.22058520089582193\n",
      "Error 0.22056804247416578\n",
      "Error 0.22055092251038264\n",
      "Error 0.220533840858665\n",
      "Error 0.22051679737398527\n",
      "Error 0.22049979191208774\n",
      "Error 0.22048282432948482\n",
      "Error 0.22046589448345122\n",
      "Error 0.22044900223201927\n",
      "Error 0.22043214743397352\n",
      "Error 0.22041532994884525\n",
      "Error 0.2203985496369077\n",
      "Error 0.22038180635917093\n",
      "Error 0.22036509997737735\n",
      "Error 0.22034843035399546\n",
      "Error 0.22033179735221647\n",
      "Error 0.22031520083594835\n",
      "Error 0.22029864066981136\n",
      "Error 0.22028211671913334\n",
      "Error 0.22026562884994472\n",
      "Error 0.220249176928974\n",
      "Error 0.22023276082364304\n",
      "Error 0.22021638040206248\n",
      "Error 0.22020003553302686\n",
      "Error 0.22018372608601017\n",
      "Error 0.22016745193116144\n",
      "Error 0.22015121293930023\n",
      "Error 0.22013500898191216\n",
      "Error 0.22011883993114467\n",
      "Error 0.2201027056598019\n",
      "Error 0.22008660604134125\n",
      "Error 0.2200705409498685\n",
      "Error 0.22005451026013395\n",
      "Error 0.22003851384752765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.22002255158807543\n",
      "Error 0.22000662335843524\n",
      "Error 0.21999072903589215\n",
      "Error 0.21997486849835451\n",
      "Error 0.21995904162435043\n",
      "Error 0.21994324829302353\n",
      "Error 0.2199274883841275\n",
      "Error 0.21991176177802477\n",
      "Error 0.21989606835567974\n",
      "Error 0.2198804079986573\n",
      "Error 0.21986478058911713\n",
      "Error 0.21984918600981115\n",
      "Error 0.2198336241440794\n",
      "Error 0.2198180948758448\n",
      "Error 0.21980259808961175\n",
      "Error 0.2197871336704605\n",
      "Error 0.21977170150404463\n",
      "Error 0.21975630147658642\n",
      "Error 0.21974093347487403\n",
      "Error 0.21972559738625724\n",
      "Error 0.21971029309864482\n",
      "Error 0.21969502050049963\n",
      "Error 0.219679779480836\n",
      "Error 0.21966456992921551\n",
      "Error 0.219649391735745\n",
      "Error 0.219634244791071\n",
      "Error 0.21961912898637811\n",
      "Error 0.21960404421338464\n",
      "Error 0.21958899036433968\n",
      "Error 0.21957396733201912\n",
      "Error 0.21955897500972332\n",
      "Error 0.21954401329127265\n",
      "Error 0.21952908207100535\n",
      "Error 0.21951418124377375\n",
      "Error 0.21949931070494055\n",
      "Error 0.21948447035037627\n",
      "Error 0.21946966007645657\n",
      "Error 0.219454879780058\n",
      "Error 0.21944012935855498\n",
      "Error 0.2194254087098177\n",
      "Error 0.2194107177322085\n",
      "Error 0.2193960563245779\n",
      "Error 0.21938142438626312\n",
      "Error 0.21936682181708433\n",
      "Error 0.21935224851734114\n",
      "Error 0.21933770438781072\n",
      "Error 0.21932318932974412\n",
      "Error 0.21930870324486373\n",
      "Error 0.2192942460353601\n",
      "Error 0.21927981760388904\n",
      "Error 0.21926541785356896\n",
      "Error 0.21925104668797793\n",
      "Error 0.21923670401115153\n",
      "Error 0.21922238972757868\n",
      "Error 0.21920810374220023\n",
      "Error 0.21919384596040511\n",
      "Error 0.21917961628802868\n",
      "Error 0.21916541463134892\n",
      "Error 0.2191512408970846\n",
      "Error 0.2191370949923924\n",
      "Error 0.21912297682486392\n",
      "Error 0.21910888630252354\n",
      "Error 0.2190948233338254\n",
      "Error 0.21908078782765106\n",
      "Error 0.21906677969330685\n",
      "Error 0.2190527988405216\n",
      "Error 0.21903884517944325\n",
      "Error 0.2190249186206373\n",
      "Error 0.21901101907508402\n",
      "Error 0.21899714645417556\n",
      "Error 0.2189833006697141\n",
      "Error 0.21896948163390947\n",
      "Error 0.21895568925937522\n",
      "Error 0.21894192345912863\n",
      "Error 0.2189281841465863\n",
      "Error 0.21891447123556323\n",
      "Error 0.21890078464026907\n",
      "Error 0.21888712427530715\n",
      "Error 0.21887349005567064\n",
      "Error 0.218859881896742\n",
      "Error 0.21884629971428965\n",
      "Error 0.21883274342446576\n",
      "Error 0.21881921294380366\n",
      "Error 0.21880570818921666\n",
      "Error 0.21879222907799503\n",
      "Error 0.2187787755278039\n",
      "Error 0.21876534745668108\n",
      "Error 0.21875194478303533\n",
      "Error 0.2187385674256433\n",
      "Error 0.21872521530364814\n",
      "Error 0.21871188833655703\n",
      "Error 0.21869858644424012\n",
      "Error 0.2186853095469258\n",
      "Error 0.21867205756520153\n",
      "Error 0.21865883042001039\n",
      "Error 0.21864562803264875\n",
      "Error 0.21863245032476514\n",
      "Error 0.21861929721835768\n",
      "Error 0.21860616863577187\n",
      "Error 0.21859306449969873\n",
      "Error 0.2185799847331737\n",
      "Error 0.2185669292595732\n",
      "Error 0.2185538980026136\n",
      "Error 0.21854089088634968\n",
      "Error 0.21852790783517104\n",
      "Error 0.21851494877380198\n",
      "Error 0.21850201362729865\n",
      "Error 0.2184891023210474\n",
      "Error 0.21847621478076298\n",
      "Error 0.21846335093248634\n",
      "Error 0.2184505107025833\n",
      "Error 0.21843769401774207\n",
      "Error 0.2184249008049725\n",
      "Error 0.2184121309916028\n",
      "Error 0.21839938450527888\n",
      "Error 0.21838666127396228\n",
      "Error 0.21837396122592803\n",
      "Error 0.21836128428976337\n",
      "Error 0.21834863039436542\n",
      "Error 0.21833599946894058\n",
      "Error 0.21832339144300134\n",
      "Error 0.21831080624636567\n",
      "Error 0.21829824380915458\n",
      "Error 0.21828570406179132\n",
      "Error 0.21827318693499861\n",
      "Error 0.21826069235979745\n",
      "Error 0.2182482202675061\n",
      "Error 0.21823577058973748\n",
      "Error 0.2182233432583983\n",
      "Error 0.21821093820568654\n",
      "Error 0.2181985553640906\n",
      "Error 0.21818619466638736\n",
      "Error 0.21817385604564093\n",
      "Error 0.2181615394352005\n",
      "Error 0.2181492447686993\n",
      "Error 0.21813697198005286\n",
      "Error 0.21812472100345753\n",
      "Error 0.21811249177338848\n",
      "Error 0.21810028422459898\n",
      "Error 0.21808809829211823\n",
      "Error 0.21807593391125044\n",
      "Error 0.21806379101757195\n",
      "Error 0.21805166954693175\n",
      "Error 0.2180395694354491\n",
      "Error 0.21802749061951138\n",
      "Error 0.2180154330357736\n",
      "Error 0.2180033966211566\n",
      "Error 0.21799138131284462\n",
      "Error 0.2179793870482865\n",
      "Error 0.21796741376519146\n",
      "Error 0.21795546140152927\n",
      "Error 0.21794352989552848\n",
      "Error 0.21793161918567489\n",
      "Error 0.21791972921070996\n",
      "Error 0.21790785990962988\n",
      "Error 0.2178960112216845\n",
      "Error 0.21788418308637486\n",
      "Error 0.21787237544345306\n",
      "Error 0.2178605882329198\n",
      "Error 0.21784882139502448\n",
      "Error 0.21783707487026205\n",
      "Error 0.2178253485993733\n",
      "Error 0.21781364252334284\n",
      "Error 0.21780195658339768\n",
      "Error 0.2177902907210063\n",
      "Error 0.21777864487787765\n",
      "Error 0.21776701899595857\n",
      "Error 0.21775541301743417\n",
      "Error 0.2177438268847256\n",
      "Error 0.21773226054048872\n",
      "Error 0.217720713927614\n",
      "Error 0.21770918698922376\n",
      "Error 0.21769767966867176\n",
      "Error 0.21768619190954222\n",
      "Error 0.2176747236556477\n",
      "Error 0.21766327485102913\n",
      "Error 0.21765184543995347\n",
      "Error 0.2176404353669132\n",
      "Error 0.2176290445766254\n",
      "Error 0.21761767301402948\n",
      "Error 0.21760632062428717\n",
      "Error 0.2175949873527808\n",
      "Error 0.217583673145112\n",
      "Error 0.2175723779471011\n",
      "Error 0.21756110170478565\n",
      "Error 0.2175498443644194\n",
      "Error 0.21753860587247115\n",
      "Error 0.217527386175623\n",
      "Error 0.21751618522077126\n",
      "Error 0.2175050029550228\n",
      "Error 0.21749383932569571\n",
      "Error 0.21748269428031736\n",
      "Error 0.2174715677666236\n",
      "Error 0.217460459732558\n",
      "Error 0.21744937012627003\n",
      "Error 0.21743829889611507\n",
      "Error 0.21742724599065247\n",
      "Error 0.21741621135864436\n",
      "Error 0.21740519494905572\n",
      "Error 0.21739419671105242\n",
      "Error 0.21738321659400053\n",
      "Error 0.21737225454746517\n",
      "Error 0.2173613105212096\n",
      "Error 0.217350384465194\n",
      "Error 0.21733947632957493\n",
      "Error 0.21732858606470398\n",
      "Error 0.21731771362112692\n",
      "Error 0.21730685894958252\n",
      "Error 0.21729602200100162\n",
      "Error 0.21728520272650703\n",
      "Error 0.21727440107741075\n",
      "Error 0.2172636170052146\n",
      "Error 0.21725285046160908\n",
      "Error 0.21724210139847133\n",
      "Error 0.2172313697678657\n",
      "Error 0.21722065552204156\n",
      "Error 0.21720995861343298\n",
      "Error 0.21719927899465796\n",
      "Error 0.21718861661851724\n",
      "Error 0.2171779714379928\n",
      "Error 0.21716734340624871\n",
      "Error 0.2171567324766283\n",
      "Error 0.21714613860265403\n",
      "Error 0.21713556173802676\n",
      "Error 0.2171250018366253\n",
      "Error 0.21711445885250455\n",
      "Error 0.21710393273989492\n",
      "Error 0.2170934234532016\n",
      "Error 0.217082930947004\n",
      "Error 0.21707245517605447\n",
      "Error 0.2170619960952774\n",
      "Error 0.2170515536597689\n",
      "Error 0.21704112782479532\n",
      "Error 0.21703071854579287\n",
      "Error 0.21702032577836655\n",
      "Error 0.2170099494782895\n",
      "Error 0.21699958960150217\n",
      "Error 0.21698924610411102\n",
      "Error 0.21697891894238844\n",
      "Error 0.21696860807277168\n",
      "Error 0.2169583134518615\n",
      "Error 0.2169480350364223\n",
      "Error 0.21693777278338106\n",
      "Error 0.21692752664982584\n",
      "Error 0.21691729659300582\n",
      "Error 0.21690708257033015\n",
      "Error 0.21689688453936745\n",
      "Error 0.2168867024578446\n",
      "Error 0.21687653628364637\n",
      "Error 0.21686638597481464\n",
      "Error 0.21685625148954735\n",
      "Error 0.21684613278619774\n",
      "Error 0.21683602982327416\n",
      "Error 0.21682594255943868\n",
      "Error 0.21681587095350716\n",
      "Error 0.21680581496444726\n",
      "Error 0.21679577455137883\n",
      "Error 0.21678574967357295\n",
      "Error 0.2167757402904506\n",
      "Error 0.21676574636158277\n",
      "Error 0.21675576784668946\n",
      "Error 0.21674580470563848\n",
      "Error 0.21673585689844535\n",
      "Error 0.2167259243852726\n",
      "Error 0.2167160071264287\n",
      "Error 0.21670610508236754\n",
      "Error 0.21669621821368748\n",
      "Error 0.21668634648113178\n",
      "Error 0.21667648984558646\n",
      "Error 0.2166666482680799\n",
      "Error 0.21665682170978337\n",
      "Error 0.21664701013200896\n",
      "Error 0.21663721349620926\n",
      "Error 0.2166274317639779\n",
      "Error 0.21661766489704642\n",
      "Error 0.21660791285728634\n",
      "Error 0.21659817560670694\n",
      "Error 0.2165884531074546\n",
      "Error 0.21657874532181287\n",
      "Error 0.21656905221220132\n",
      "Error 0.21655937374117493\n",
      "Error 0.2165497098714238\n",
      "Error 0.21654006056577207\n",
      "Error 0.2165304257871779\n",
      "Error 0.21652080549873198\n",
      "Error 0.21651119966365773\n",
      "Error 0.21650160824531017\n",
      "Error 0.21649203120717578\n",
      "Error 0.2164824685128713\n",
      "Error 0.2164729201261439\n",
      "Error 0.2164633860108694\n",
      "Error 0.2164538661310532\n",
      "Error 0.21644436045082846\n",
      "Error 0.21643486893445604\n",
      "Error 0.21642539154632376\n",
      "Error 0.21641592825094585\n",
      "Error 0.21640647901296273\n",
      "Error 0.21639704379713942\n",
      "Error 0.2163876225683663\n",
      "Error 0.21637821529165766\n",
      "Error 0.2163688219321514\n",
      "Error 0.21635944245510846\n",
      "Error 0.21635007682591204\n",
      "Error 0.21634072501006735\n",
      "Error 0.21633138697320065\n",
      "Error 0.21632206268105975\n",
      "Error 0.21631275209951184\n",
      "Error 0.21630345519454447\n",
      "Error 0.21629417193226402\n",
      "Error 0.2162849022788953\n",
      "Error 0.2162756462007817\n",
      "Error 0.2162664036643836\n",
      "Error 0.21625717463627883\n",
      "Error 0.2162479590831617\n",
      "Error 0.21623875697184208\n",
      "Error 0.21622956826924558\n",
      "Error 0.21622039294241296\n",
      "Error 0.2162112309584991\n",
      "Error 0.2162020822847726\n",
      "Error 0.21619294688861582\n",
      "Error 0.21618382473752393\n",
      "Error 0.21617471579910422\n",
      "Error 0.21616562004107626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.2161565374312706\n",
      "Error 0.21614746793762896\n",
      "Error 0.21613841152820346\n",
      "Error 0.21612936817115586\n",
      "Error 0.21612033783475743\n",
      "Error 0.21611132048738835\n",
      "Error 0.21610231609753755\n",
      "Error 0.2160933246338018\n",
      "Error 0.21608434606488497\n",
      "Error 0.21607538035959842\n",
      "Error 0.21606642748685972\n",
      "Error 0.216057487415693\n",
      "Error 0.21604856011522697\n",
      "Error 0.21603964555469685\n",
      "Error 0.21603074370344116\n",
      "Error 0.21602185453090367\n",
      "Error 0.2160129780066316\n",
      "Error 0.2160041141002754\n",
      "Error 0.21599526278158857\n",
      "Error 0.2159864240204262\n",
      "Error 0.2159775977867467\n",
      "Error 0.21596878405060904\n",
      "Error 0.21595998278217335\n",
      "Error 0.2159511939517008\n",
      "Error 0.21594241752955196\n",
      "Error 0.21593365348618843\n",
      "Error 0.2159249017921701\n",
      "Error 0.21591616241815598\n",
      "Error 0.2159074353349036\n",
      "Error 0.2158987205132684\n",
      "Error 0.21589001792420368\n",
      "Error 0.2158813275387596\n",
      "Error 0.21587264932808375\n",
      "Error 0.21586398326341943\n",
      "Error 0.2158553293161062\n",
      "Error 0.21584668745757854\n",
      "Error 0.21583805765936695\n",
      "Error 0.21582943989309608\n",
      "Error 0.21582083413048508\n",
      "Error 0.21581224034334676\n",
      "Error 0.2158036585035875\n",
      "Error 0.21579508858320695\n",
      "Error 0.21578653055429758\n",
      "Error 0.2157779843890435\n",
      "Error 0.21576945005972165\n",
      "Error 0.2157609275386998\n",
      "Error 0.21575241679843696\n",
      "Error 0.21574391781148278\n",
      "Error 0.21573543055047795\n",
      "Error 0.21572695498815192\n",
      "Error 0.21571849109732474\n",
      "Error 0.2157100388509053\n",
      "Error 0.21570159822189164\n",
      "Error 0.2156931691833695\n",
      "Error 0.21568475170851337\n",
      "Error 0.2156763457705852\n",
      "Error 0.21566795134293432\n",
      "Error 0.2156595683989968\n",
      "Error 0.21565119691229576\n",
      "Error 0.2156428368564402\n",
      "Error 0.215634488205125\n",
      "Error 0.21562615093213072\n",
      "Error 0.21561782501132284\n",
      "Error 0.21560951041665227\n",
      "Error 0.21560120712215364\n",
      "Error 0.2155929151019455\n",
      "Error 0.215584634330231\n",
      "Error 0.2155763647812964\n",
      "Error 0.21556810642951044\n",
      "Error 0.2155598592493248\n",
      "Error 0.21555162321527388\n",
      "Error 0.21554339830197433\n",
      "Error 0.2155351844841233\n",
      "Error 0.21552698173650015\n",
      "Error 0.21551879003396496\n",
      "Error 0.21551060935145866\n",
      "Error 0.21550243966400257\n",
      "Error 0.21549428094669745\n",
      "Error 0.2154861331747242\n",
      "Error 0.21547799632334286\n",
      "Error 0.21546987036789247\n",
      "Error 0.21546175528379102\n",
      "Error 0.21545365104653444\n",
      "Error 0.21544555763169715\n",
      "Error 0.21543747501493052\n",
      "Error 0.21542940317196427\n",
      "Error 0.21542134207860433\n",
      "Error 0.215413291710734\n",
      "Error 0.2154052520443126\n",
      "Error 0.21539722305537595\n",
      "Error 0.21538920472003528\n",
      "Error 0.21538119701447794\n",
      "Error 0.2153731999149655\n",
      "Error 0.21536521339783526\n",
      "Error 0.2153572374394991\n",
      "Error 0.21534927201644294\n",
      "Error 0.215341317105226\n",
      "Error 0.21533337268248273\n",
      "Error 0.21532543872491966\n",
      "Error 0.2153175152093168\n",
      "Error 0.2153096021125272\n",
      "Error 0.21530169941147603\n",
      "Error 0.21529380708316087\n",
      "Error 0.21528592510465122\n",
      "Error 0.2152780534530879\n",
      "Error 0.2152701921056837\n",
      "Error 0.21526234103972192\n",
      "Error 0.2152545002325564\n",
      "Error 0.2152466696616123\n",
      "Error 0.21523884930438414\n",
      "Error 0.21523103913843672\n",
      "Error 0.21522323914140462\n",
      "Error 0.21521544929099123\n",
      "Error 0.2152076695649697\n",
      "Error 0.21519989994118166\n",
      "Error 0.21519214039753706\n",
      "Error 0.21518439091201444\n",
      "Error 0.21517665146266\n",
      "Error 0.21516892202758794\n",
      "Error 0.21516120258497945\n",
      "Error 0.2151534931130834\n",
      "Error 0.21514579359021538\n",
      "Error 0.2151381039947576\n",
      "Error 0.21513042430515888\n",
      "Error 0.21512275449993332\n",
      "Error 0.2151150945576616\n",
      "Error 0.21510744445698995\n",
      "Error 0.21509980417662958\n",
      "Error 0.21509217369535705\n",
      "Error 0.21508455299201365\n",
      "Error 0.21507694204550526\n",
      "Error 0.21506934083480217\n",
      "Error 0.21506174933893848\n",
      "Error 0.21505416753701218\n",
      "Error 0.21504659540818477\n",
      "Error 0.215039032931681\n",
      "Error 0.21503148008678902\n",
      "Error 0.2150239368528591\n",
      "Error 0.21501640320930476\n",
      "Error 0.21500887913560157\n",
      "Error 0.215001364611287\n",
      "Error 0.21499385961596032\n",
      "Error 0.21498636412928313\n",
      "Error 0.21497887813097724\n",
      "Error 0.21497140160082645\n",
      "Error 0.21496393451867482\n",
      "Error 0.21495647686442737\n",
      "Error 0.2149490286180495\n",
      "Error 0.21494158975956662\n",
      "Error 0.21493416026906448\n",
      "Error 0.2149267401266876\n",
      "Error 0.2149193293126413\n",
      "Error 0.21491192780718885\n",
      "Error 0.21490453559065342\n",
      "Error 0.2148971526434166\n",
      "Error 0.2148897789459184\n",
      "Error 0.2148824144786572\n",
      "Error 0.2148750592221899\n",
      "Error 0.21486771315713088\n",
      "Error 0.2148603762641521\n",
      "Error 0.2148530485239834\n",
      "Error 0.21484572991741122\n",
      "Error 0.21483842042527942\n",
      "Error 0.21483112002848878\n",
      "Error 0.21482382870799585\n",
      "Error 0.21481654644481457\n",
      "Error 0.21480927322001434\n",
      "Error 0.2148020090147204\n",
      "Error 0.21479475381011442\n",
      "Error 0.2147875075874326\n",
      "Error 0.21478027032796704\n",
      "Error 0.2147730420130649\n",
      "Error 0.21476582262412766\n",
      "Error 0.21475861214261185\n",
      "Error 0.21475141055002878\n",
      "Error 0.21474421782794315\n",
      "Error 0.21473703395797447\n",
      "Error 0.2147298589217959\n",
      "Error 0.2147226927011338\n",
      "Error 0.21471553527776846\n",
      "Error 0.21470838663353303\n",
      "Error 0.21470124675031382\n",
      "Error 0.21469411561004975\n",
      "Error 0.21468699319473292\n",
      "Error 0.21467987948640732\n",
      "Error 0.21467277446716895\n",
      "Error 0.21466567811916698\n",
      "Error 0.21465859042460095\n",
      "Error 0.21465151136572308\n",
      "Error 0.21464444092483653\n",
      "Error 0.214637379084296\n",
      "Error 0.21463032582650707\n",
      "Error 0.21462328113392587\n",
      "Error 0.21461624498906035\n",
      "Error 0.21460921737446736\n",
      "Error 0.21460219827275534\n",
      "Error 0.21459518766658198\n",
      "Error 0.21458818553865563\n",
      "Error 0.21458119187173388\n",
      "Error 0.21457420664862398\n",
      "Error 0.21456722985218266\n",
      "Error 0.2145602614653158\n",
      "Error 0.21455330147097804\n",
      "Error 0.2145463498521734\n",
      "Error 0.21453940659195414\n",
      "Error 0.21453247167342093\n",
      "Error 0.21452554507972277\n",
      "Error 0.21451862679405723\n",
      "Error 0.21451171679966913\n",
      "Error 0.21450481507985147\n",
      "Error 0.2144979216179447\n",
      "Error 0.2144910363973366\n",
      "Error 0.21448415940146232\n",
      "Error 0.214477290613804\n",
      "Error 0.2144704300178908\n",
      "Error 0.21446357759729823\n",
      "Error 0.21445673333564885\n",
      "Error 0.2144498972166112\n",
      "Error 0.21444306922390016\n",
      "Error 0.21443624934127675\n",
      "Error 0.2144294375525475\n",
      "Error 0.21442263384156485\n",
      "Error 0.21441583819222693\n",
      "Error 0.21440905058847717\n",
      "Error 0.21440227101430395\n",
      "Error 0.21439549945374092\n",
      "Error 0.2143887358908667\n",
      "Error 0.21438198030980415\n",
      "Error 0.21437523269472095\n",
      "Error 0.21436849302982947\n",
      "Error 0.2143617612993863\n",
      "Error 0.21435503748769116\n",
      "Error 0.21434832157908867\n",
      "Error 0.21434161355796713\n",
      "Error 0.21433491340875785\n",
      "Error 0.2143282211159359\n",
      "Error 0.21432153666401937\n",
      "Error 0.21431486003757022\n",
      "Error 0.21430819122119274\n",
      "Error 0.21430153019953405\n",
      "Error 0.2142948769572841\n",
      "Error 0.21428823147917528\n",
      "Error 0.2142815937499823\n",
      "Error 0.2142749637545222\n",
      "Error 0.2142683414776539\n",
      "Error 0.21426172690427828\n",
      "Error 0.21425512001933816\n",
      "Error 0.2142485208078174\n",
      "Error 0.21424192925474178\n",
      "Error 0.2142353453451787\n",
      "Error 0.21422876906423588\n",
      "Error 0.2142222003970627\n",
      "Error 0.21421563932884924\n",
      "Error 0.21420908584482592\n",
      "Error 0.2142025399302642\n",
      "Error 0.21419600157047597\n",
      "Error 0.2141894707508131\n",
      "Error 0.21418294745666794\n",
      "Error 0.21417643167347258\n",
      "Error 0.21416992338669885\n",
      "Error 0.21416342258185864\n",
      "Error 0.2141569292445033\n",
      "Error 0.2141504433602238\n",
      "Error 0.21414396491464993\n",
      "Error 0.214137493893451\n",
      "Error 0.21413103028233538\n",
      "Error 0.2141245740670501\n",
      "Error 0.2141181252333811\n",
      "Error 0.21411168376715278\n",
      "Error 0.2141052496542279\n",
      "Error 0.21409882288050835\n",
      "Error 0.21409240343193367\n",
      "Error 0.2140859912944809\n",
      "Error 0.21407958645416594\n",
      "Error 0.2140731888970422\n",
      "Error 0.21406679860920094\n",
      "Error 0.2140604155767703\n",
      "Error 0.2140540397859167\n",
      "Error 0.21404767122284313\n",
      "Error 0.2140413098737902\n",
      "Error 0.21403495572503503\n",
      "Error 0.2140286087628923\n",
      "Error 0.2140222689737133\n",
      "Error 0.21401593634388574\n",
      "Error 0.21400961085983378\n",
      "Error 0.2140032925080178\n",
      "Error 0.21399698127493547\n",
      "Error 0.21399067714711956\n",
      "Error 0.21398438011113913\n",
      "Error 0.21397809015359898\n",
      "Error 0.2139718072611403\n",
      "Error 0.213965531420439\n",
      "Error 0.21395926261820747\n",
      "Error 0.2139530008411926\n",
      "Error 0.21394674607617703\n",
      "Error 0.2139404983099787\n",
      "Error 0.2139342575294502\n",
      "Error 0.2139280237214794\n",
      "Error 0.2139217968729886\n",
      "Error 0.21391557697093475\n",
      "Error 0.21390936400231012\n",
      "Error 0.21390315795414047\n",
      "Error 0.21389695881348608\n",
      "Error 0.21389076656744188\n",
      "Error 0.21388458120313625\n",
      "Error 0.21387840270773206\n",
      "Error 0.21387223106842618\n",
      "Error 0.21386606627244859\n",
      "Error 0.21385990830706314\n",
      "Error 0.21385375715956742\n",
      "Error 0.213847612817292\n",
      "Error 0.213841475267601\n",
      "Error 0.2138353444978919\n",
      "Error 0.2138292204955948\n",
      "Error 0.21382310324817314\n",
      "Error 0.21381699274312282\n",
      "Error 0.2138108889679729\n",
      "Error 0.21380479191028434\n",
      "Error 0.21379870155765124\n",
      "Error 0.2137926178977001\n",
      "Error 0.2137865409180893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.21378047060650954\n",
      "Error 0.21377440695068395\n",
      "Error 0.21376834993836674\n",
      "Error 0.21376229955734524\n",
      "Error 0.21375625579543733\n",
      "Error 0.21375021864049287\n",
      "Error 0.21374418808039358\n",
      "Error 0.21373816410305252\n",
      "Error 0.21373214669641385\n",
      "Error 0.21372613584845288\n",
      "Error 0.21372013154717653\n",
      "Error 0.21371413378062204\n",
      "Error 0.21370814253685824\n",
      "Error 0.2137021578039843\n",
      "Error 0.21369617957012982\n",
      "Error 0.21369020782345577\n",
      "Error 0.21368424255215301\n",
      "Error 0.2136782837444433\n",
      "Error 0.21367233138857805\n",
      "Error 0.21366638547283925\n",
      "Error 0.21366044598553877\n",
      "Error 0.21365451291501913\n",
      "Error 0.2136485862496515\n",
      "Error 0.2136426659778381\n",
      "Error 0.21363675208800997\n",
      "Error 0.21363084456862821\n",
      "Error 0.21362494340818344\n",
      "Error 0.21361904859519534\n",
      "Error 0.21361316011821313\n",
      "Error 0.21360727796581505\n",
      "Error 0.2136014021266087\n",
      "Error 0.21359553258923017\n",
      "Error 0.2135896693423455\n",
      "Error 0.21358381237464838\n",
      "Error 0.21357796167486193\n",
      "Error 0.21357211723173783\n",
      "Error 0.21356627903405595\n",
      "Error 0.213560447070625\n",
      "Error 0.21355462133028175\n",
      "Error 0.2135488018018914\n",
      "Error 0.2135429884743476\n",
      "Error 0.21353718133657149\n",
      "Error 0.21353138037751243\n",
      "Error 0.21352558558614768\n",
      "Error 0.21351979695148252\n",
      "Error 0.2135140144625495\n",
      "Error 0.2135082381084093\n",
      "Error 0.21350246787814942\n",
      "Error 0.2134967037608857\n",
      "Error 0.2134909457457605\n",
      "Error 0.21348519382194425\n",
      "Error 0.21347944797863386\n",
      "Error 0.2134737082050536\n",
      "Error 0.21346797449045476\n",
      "Error 0.21346224682411535\n",
      "Error 0.21345652519534034\n",
      "Error 0.21345080959346166\n",
      "Error 0.21344510000783767\n",
      "Error 0.21343939642785323\n",
      "Error 0.2134336988429198\n",
      "Error 0.21342800724247524\n",
      "Error 0.21342232161598354\n",
      "Error 0.21341664195293514\n",
      "Error 0.21341096824284636\n",
      "Error 0.21340530047525993\n",
      "Error 0.21339963863974426\n",
      "Error 0.21339398272589394\n",
      "Error 0.21338833272332888\n",
      "Error 0.21338268862169513\n",
      "Error 0.21337705041066424\n",
      "Error 0.21337141807993348\n",
      "Error 0.2133657916192254\n",
      "Error 0.21336017101828803\n",
      "Error 0.21335455626689454\n",
      "Error 0.2133489473548437\n",
      "Error 0.21334334427195886\n",
      "Error 0.21333774700808927\n",
      "Error 0.21333215555310825\n",
      "Error 0.21332656989691468\n",
      "Error 0.21332099002943233\n",
      "Error 0.2133154159406089\n",
      "Error 0.21330984762041788\n",
      "Error 0.21330428505885662\n",
      "Error 0.21329872824594703\n",
      "Error 0.2132931771717359\n",
      "Error 0.213287631826294\n",
      "Error 0.21328209219971664\n",
      "Error 0.213276558282123\n",
      "Error 0.21327103006365666\n",
      "Error 0.2132655075344852\n",
      "Error 0.21325999068480023\n",
      "Error 0.21325447950481707\n",
      "Error 0.21324897398477524\n",
      "Error 0.21324347411493744\n",
      "Error 0.21323797988559096\n",
      "Error 0.21323249128704572\n",
      "Error 0.2132270083096357\n",
      "Error 0.21322153094371815\n",
      "Error 0.21321605917967398\n",
      "Error 0.2132105930079074\n",
      "Error 0.21320513241884567\n",
      "Error 0.2131996774029389\n",
      "Error 0.2131942279506609\n",
      "Error 0.21318878405250805\n",
      "Error 0.2131833456989999\n",
      "Error 0.21317791288067886\n",
      "Error 0.2131724855881107\n",
      "Error 0.2131670638118827\n",
      "Error 0.21316164754260583\n",
      "Error 0.21315623677091317\n",
      "Error 0.2131508314874603\n",
      "Error 0.2131454316829257\n",
      "Error 0.21314003734800985\n",
      "Error 0.21313464847343583\n",
      "Error 0.2131292650499484\n",
      "Error 0.21312388706831525\n",
      "Error 0.2131185145193256\n",
      "Error 0.21311314739379122\n",
      "Error 0.21310778568254504\n",
      "Error 0.21310242937644303\n",
      "Error 0.213097078466362\n",
      "Error 0.21309173294320086\n",
      "Error 0.21308639279788033\n",
      "Error 0.21308105802134322\n",
      "Error 0.21307572860455296\n",
      "Error 0.21307040453849493\n",
      "Error 0.21306508581417588\n",
      "Error 0.21305977242262414\n",
      "Error 0.21305446435488934\n",
      "Error 0.21304916160204207\n",
      "Error 0.2130438641551744\n",
      "Error 0.21303857200539902\n",
      "Error 0.21303328514385006\n",
      "Error 0.21302800356168294\n",
      "Error 0.21302272725007323\n",
      "Error 0.21301745620021806\n",
      "Error 0.2130121904033348\n",
      "Error 0.21300692985066189\n",
      "Error 0.21300167453345842\n",
      "Error 0.21299642444300376\n",
      "Error 0.21299117957059827\n",
      "Error 0.21298593990756248\n",
      "Error 0.2129807054452375\n",
      "Error 0.21297547617498447\n",
      "Error 0.21297025208818546\n",
      "Error 0.21296503317624219\n",
      "Error 0.21295981943057624\n",
      "Error 0.21295461084263034\n",
      "Error 0.21294940740386692\n",
      "Error 0.21294420910576756\n",
      "Error 0.2129390159398346\n",
      "Error 0.2129338278975899\n",
      "Error 0.21292864497057568\n",
      "Error 0.21292346715035312\n",
      "Error 0.2129182944285032\n",
      "Error 0.2129131267966272\n",
      "Error 0.21290796424634553\n",
      "Error 0.21290280676929765\n",
      "Error 0.21289765435714317\n",
      "Error 0.21289250700156087\n",
      "Error 0.2128873646942488\n",
      "Error 0.21288222742692434\n",
      "Error 0.21287709519132383\n",
      "Error 0.21287196797920319\n",
      "Error 0.21286684578233714\n",
      "Error 0.21286172859251984\n",
      "Error 0.2128566164015636\n",
      "Error 0.21285150920130053\n",
      "Error 0.21284640698358137\n",
      "Error 0.2128413097402752\n",
      "Error 0.21283621746327058\n",
      "Error 0.21283113014447425\n",
      "Error 0.21282604777581188\n",
      "Error 0.2128209703492277\n",
      "Error 0.21281589785668414\n",
      "Error 0.21281083029016265\n",
      "Error 0.21280576764166287\n",
      "Error 0.21280070990320285\n",
      "Error 0.21279565706681827\n",
      "Error 0.21279060912456446\n",
      "Error 0.21278556606851384\n",
      "Error 0.21278052789075722\n",
      "Error 0.2127754945834039\n",
      "Error 0.2127704661385808\n",
      "Error 0.21276544254843324\n",
      "Error 0.21276042380512378\n",
      "Error 0.21275540990083383\n",
      "Error 0.2127504008277617\n",
      "Error 0.2127453965781243\n",
      "Error 0.21274039714415585\n",
      "Error 0.21273540251810794\n",
      "Error 0.2127304126922504\n",
      "Error 0.21272542765887054\n",
      "Error 0.21272044741027268\n",
      "Error 0.2127154719387792\n",
      "Error 0.21271050123672947\n",
      "Error 0.21270553529648062\n",
      "Error 0.21270057411040671\n",
      "Error 0.2126956176708993\n",
      "Error 0.21269066597036726\n",
      "Error 0.21268571900123634\n",
      "Error 0.2126807767559497\n",
      "Error 0.21267583922696734\n",
      "Error 0.2126709064067664\n",
      "Error 0.21266597828784098\n",
      "Error 0.21266105486270193\n",
      "Error 0.2126561361238773\n",
      "Error 0.2126512220639115\n",
      "Error 0.2126463126753661\n",
      "Error 0.21264140795081948\n",
      "Error 0.21263650788286612\n",
      "Error 0.21263161246411796\n",
      "Error 0.2126267216872027\n",
      "Error 0.21262183554476494\n",
      "Error 0.21261695402946587\n",
      "Error 0.21261207713398292\n",
      "Error 0.21260720485101023\n",
      "Error 0.21260233717325736\n",
      "Error 0.21259747409345137\n",
      "Error 0.21259261560433496\n",
      "Error 0.21258776169866683\n",
      "Error 0.21258291236922228\n",
      "Error 0.21257806760879244\n",
      "Error 0.21257322741018472\n",
      "Error 0.21256839176622205\n",
      "Error 0.21256356066974386\n",
      "Error 0.2125587341136054\n",
      "Error 0.21255391209067778\n",
      "Error 0.21254909459384747\n",
      "Error 0.21254428161601768\n",
      "Error 0.21253947315010624\n",
      "Error 0.2125346691890475\n",
      "Error 0.21252986972579127\n",
      "Error 0.2125250747533028\n",
      "Error 0.21252028426456288\n",
      "Error 0.2125154982525677\n",
      "Error 0.21251071671032937\n",
      "Error 0.21250593963087533\n",
      "Error 0.21250116700724794\n",
      "Error 0.21249639883250562\n",
      "Error 0.21249163509972133\n",
      "Error 0.2124868758019836\n",
      "Error 0.21248212093239646\n",
      "Error 0.21247737048407855\n",
      "Error 0.21247262445016402\n",
      "Error 0.21246788282380208\n",
      "Error 0.21246314559815688\n",
      "Error 0.21245841276640753\n",
      "Error 0.21245368432174802\n",
      "Error 0.21244896025738744\n",
      "Error 0.21244424056654956\n",
      "Error 0.21243952524247317\n",
      "Error 0.2124348142784116\n",
      "Error 0.2124301076676333\n",
      "Error 0.21242540540342106\n",
      "Error 0.21242070747907257\n",
      "Error 0.2124160138878998\n",
      "Error 0.2124113246232297\n",
      "Error 0.21240663967840337\n",
      "Error 0.21240195904677645\n",
      "Error 0.2123972827217197\n",
      "Error 0.2123926106966172\n",
      "Error 0.21238794296486846\n",
      "Error 0.21238327951988648\n",
      "Error 0.2123786203550991\n",
      "Error 0.2123739654639479\n",
      "Error 0.21236931483988902\n",
      "Error 0.21236466847639304\n",
      "Error 0.2123600263669442\n",
      "Error 0.21235538850504065\n",
      "Error 0.2123507548841951\n",
      "Error 0.21234612549793427\n",
      "Error 0.21234150033979876\n",
      "Error 0.2123368794033428\n",
      "Error 0.21233226268213468\n",
      "Error 0.21232765016975663\n",
      "Error 0.21232304185980486\n",
      "Error 0.21231843774588866\n",
      "Error 0.2123138378216321\n",
      "Error 0.2123092420806723\n",
      "Error 0.2123046505166601\n",
      "Error 0.21230006312326002\n",
      "Error 0.21229547989415015\n",
      "Error 0.21229090082302213\n",
      "Error 0.21228632590358126\n",
      "Error 0.212281755129546\n",
      "Error 0.2122771884946486\n",
      "Error 0.2122726259926347\n",
      "Error 0.21226806761726275\n",
      "Error 0.21226351336230528\n",
      "Error 0.2122589632215477\n",
      "Error 0.2122544171887888\n",
      "Error 0.21224987525784028\n",
      "Error 0.21224533742252755\n",
      "Error 0.21224080367668868\n",
      "Error 0.21223627401417536\n",
      "Error 0.21223174842885162\n",
      "Error 0.21222722691459545\n",
      "Error 0.21222270946529687\n",
      "Error 0.2122181960748597\n",
      "Error 0.21221368673720012\n",
      "Error 0.21220918144624745\n",
      "Error 0.21220468019594393\n",
      "Error 0.2122001829802445\n",
      "Error 0.21219568979311715\n",
      "Error 0.21219120062854185\n",
      "Error 0.21218671548051224\n",
      "Error 0.21218223434303427\n",
      "Error 0.21217775721012605\n",
      "Error 0.21217328407581926\n",
      "Error 0.21216881493415757\n",
      "Error 0.21216434977919713\n",
      "Error 0.2121598886050068\n",
      "Error 0.21215543140566812\n",
      "Error 0.21215097817527448\n",
      "Error 0.21214652890793237\n",
      "Error 0.21214208359776004\n",
      "Error 0.21213764223888862\n",
      "Error 0.21213320482546102\n",
      "Error 0.21212877135163294\n",
      "Error 0.21212434181157197\n",
      "Error 0.21211991619945797\n",
      "Error 0.21211549450948305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.21211107673585183\n",
      "Error 0.21210666287278018\n",
      "Error 0.21210225291449664\n",
      "Error 0.21209784685524183\n",
      "Error 0.212093444689268\n",
      "Error 0.21208904641083987\n",
      "Error 0.21208465201423357\n",
      "Error 0.21208026149373768\n",
      "Error 0.21207587484365215\n",
      "Error 0.2120714920582895\n",
      "Error 0.21206711313197327\n",
      "Error 0.21206273805903925\n",
      "Error 0.21205836683383503\n",
      "Error 0.21205399945071934\n",
      "Error 0.21204963590406334\n",
      "Error 0.21204527618824964\n",
      "Error 0.21204092029767224\n",
      "Error 0.21203656822673697\n",
      "Error 0.21203221996986094\n",
      "Error 0.21202787552147334\n",
      "Error 0.2120235348760143\n",
      "Error 0.2120191980279357\n",
      "Error 0.21201486497170077\n",
      "Error 0.21201053570178408\n",
      "Error 0.21200621021267238\n",
      "Error 0.21200188849886253\n",
      "Error 0.21199757055486348\n",
      "Error 0.2119932563751953\n",
      "Error 0.2119889459543892\n",
      "Error 0.21198463928698782\n",
      "Error 0.21198033636754504\n",
      "Error 0.21197603719062585\n",
      "Error 0.21197174175080616\n",
      "Error 0.2119674500426732\n",
      "Error 0.2119631620608254\n",
      "Error 0.21195887779987194\n",
      "Error 0.2119545972544333\n",
      "Error 0.21195032041914114\n",
      "Error 0.21194604728863733\n",
      "Error 0.21194177785757534\n",
      "Error 0.2119375121206195\n",
      "Error 0.21193325007244473\n",
      "Error 0.21192899170773707\n",
      "Error 0.21192473702119324\n",
      "Error 0.21192048600752095\n",
      "Error 0.21191623866143824\n",
      "Error 0.21191199497767427\n",
      "Error 0.21190775495096906\n",
      "Error 0.21190351857607279\n",
      "Error 0.21189928584774703\n",
      "Error 0.21189505676076315\n",
      "Error 0.21189083130990355\n",
      "Error 0.2118866094899614\n",
      "Error 0.21188239129573966\n",
      "Error 0.21187817672205278\n",
      "Error 0.2118739657637251\n",
      "Error 0.2118697584155916\n",
      "Error 0.21186555467249785\n",
      "Error 0.2118613545292995\n",
      "Error 0.21185715798086235\n",
      "Error 0.21185296502206324\n",
      "Error 0.211848775647789\n",
      "Error 0.211844589852937\n",
      "Error 0.2118404076324142\n",
      "Error 0.21183622898113863\n",
      "Error 0.21183205389403792\n",
      "Error 0.2118278823660505\n",
      "Error 0.2118237143921245\n",
      "Error 0.2118195499672179\n",
      "Error 0.21181538908629957\n",
      "Error 0.21181123174434813\n",
      "Error 0.2118070779363522\n",
      "Error 0.21180292765731043\n",
      "Error 0.21179878090223148\n",
      "Error 0.21179463766613393\n",
      "Error 0.21179049794404659\n",
      "Error 0.21178636173100807\n",
      "Error 0.21178222902206664\n",
      "Error 0.21177809981228068\n",
      "Error 0.21177397409671855\n",
      "Error 0.21176985187045821\n",
      "Error 0.21176573312858765\n",
      "Error 0.21176161786620415\n",
      "Error 0.21175750607841515\n",
      "Error 0.21175339776033808\n",
      "Error 0.2117492929070995\n",
      "Error 0.2117451915138362\n",
      "Error 0.21174109357569373\n",
      "Error 0.21173699908782856\n",
      "Error 0.2117329080454055\n",
      "Error 0.21172882044359972\n",
      "Error 0.21172473627759567\n",
      "Error 0.21172065554258773\n",
      "Error 0.21171657823377918\n",
      "Error 0.21171250434638306\n",
      "Error 0.2117084338756218\n",
      "Error 0.21170436681672739\n",
      "Error 0.2117003031649412\n",
      "Error 0.21169624291551395\n",
      "Error 0.21169218606370582\n",
      "Error 0.21168813260478606\n",
      "Error 0.21168408253403345\n",
      "Error 0.21168003584673584\n",
      "Error 0.21167599253819094\n",
      "Error 0.21167195260370505\n",
      "Error 0.21166791603859375\n",
      "Error 0.21166388283818235\n",
      "Error 0.21165985299780493\n",
      "Error 0.21165582651280465\n",
      "Error 0.21165180337853395\n",
      "Error 0.21164778359035413\n",
      "Error 0.2116437671436361\n",
      "Error 0.2116397540337593\n",
      "Error 0.21163574425611248\n",
      "Error 0.21163173780609346\n",
      "Error 0.21162773467910853\n",
      "Error 0.2116237348705737\n",
      "Error 0.21161973837591339\n",
      "Error 0.21161574519056128\n",
      "Error 0.21161175530996\n",
      "Error 0.21160776872956039\n",
      "Error 0.21160378544482267\n",
      "Error 0.21159980545121568\n",
      "Error 0.2115958287442176\n",
      "Error 0.21159185531931451\n",
      "Error 0.2115878851720025\n",
      "Error 0.21158391829778486\n",
      "Error 0.2115799546921749\n",
      "Error 0.21157599435069413\n",
      "Error 0.21157203726887225\n",
      "Error 0.2115680834422486\n",
      "Error 0.2115641328663705\n",
      "Error 0.21156018553679382\n",
      "Error 0.21155624144908347\n",
      "Error 0.21155230059881247\n",
      "Error 0.21154836298156293\n",
      "Error 0.2115444285929251\n",
      "Error 0.2115404974284974\n",
      "Error 0.21153656948388755\n",
      "Error 0.21153264475471137\n",
      "Error 0.21152872323659272\n",
      "Error 0.2115248049251645\n",
      "Error 0.21152088981606793\n",
      "Error 0.211516977904952\n",
      "Error 0.2115130691874747\n",
      "Error 0.21150916365930192\n",
      "Error 0.21150526131610842\n",
      "Error 0.21150136215357668\n",
      "Error 0.21149746616739779\n",
      "Error 0.21149357335327107\n",
      "Error 0.21148968370690402\n",
      "Error 0.21148579722401237\n",
      "Error 0.21148191390031976\n",
      "Error 0.21147803373155832\n",
      "Error 0.21147415671346845\n",
      "Error 0.21147028284179833\n",
      "Error 0.21146641211230466\n",
      "Error 0.21146254452075194\n",
      "Error 0.21145868006291288\n",
      "Error 0.2114548187345679\n",
      "Error 0.21145096053150594\n",
      "Error 0.21144710544952364\n",
      "Error 0.21144325348442589\n",
      "Error 0.2114394046320253\n",
      "Error 0.21143555888814247\n",
      "Error 0.21143171624860613\n",
      "Error 0.21142787670925273\n",
      "Error 0.2114240402659267\n",
      "Error 0.21142020691448032\n",
      "Error 0.21141637665077395\n",
      "Error 0.21141254947067517\n",
      "Error 0.21140872537006022\n",
      "Error 0.21140490434481238\n",
      "Error 0.21140108639082317\n",
      "Error 0.21139727150399185\n",
      "Error 0.211393459680225\n",
      "Error 0.21138965091543752\n",
      "Error 0.21138584520555176\n",
      "Error 0.21138204254649745\n",
      "Error 0.21137824293421245\n",
      "Error 0.21137444636464223\n",
      "Error 0.21137065283373974\n",
      "Error 0.21136686233746538\n",
      "Error 0.21136307487178727\n",
      "Error 0.2113592904326813\n",
      "Error 0.21135550901613098\n",
      "Error 0.21135173061812668\n",
      "Error 0.21134795523466696\n",
      "Error 0.21134418286175777\n",
      "Error 0.21134041349541258\n",
      "Error 0.21133664713165187\n",
      "Error 0.21133288376650392\n",
      "Error 0.2113291233960047\n",
      "Error 0.21132536601619706\n",
      "Error 0.2113216116231317\n",
      "Error 0.2113178602128664\n",
      "Error 0.21131411178146614\n",
      "Error 0.211310366325004\n",
      "Error 0.21130662383955934\n",
      "Error 0.21130288432121977\n",
      "Error 0.21129914776607941\n",
      "Error 0.21129541417024056\n",
      "Error 0.21129168352981145\n",
      "Error 0.21128795584090895\n",
      "Error 0.21128423109965627\n",
      "Error 0.21128050930218417\n",
      "Error 0.2112767904446305\n",
      "Error 0.21127307452314015\n",
      "Error 0.21126936153386552\n",
      "Error 0.21126565147296544\n",
      "Error 0.21126194433660656\n",
      "Error 0.21125824012096248\n",
      "Error 0.21125453882221362\n",
      "Error 0.21125084043654763\n",
      "Error 0.21124714496015945\n",
      "Error 0.21124345238925063\n",
      "Error 0.21123976272002962\n",
      "Error 0.21123607594871271\n",
      "Error 0.21123239207152208\n",
      "Error 0.21122871108468777\n",
      "Error 0.21122503298444623\n",
      "Error 0.21122135776704085\n",
      "Error 0.21121768542872235\n",
      "Error 0.21121401596574793\n",
      "Error 0.211210349374382\n",
      "Error 0.21120668565089531\n",
      "Error 0.21120302479156616\n",
      "Error 0.21119936679267917\n",
      "Error 0.21119571165052609\n",
      "Error 0.21119205936140512\n",
      "Error 0.21118840992162138\n",
      "Error 0.21118476332748695\n",
      "Error 0.21118111957532051\n",
      "Error 0.21117747866144732\n",
      "Error 0.21117384058219973\n",
      "Error 0.2111702053339164\n",
      "Error 0.21116657291294333\n",
      "Error 0.21116294331563204\n",
      "Error 0.211159316538342\n",
      "Error 0.2111556925774382\n",
      "Error 0.21115207142929301\n",
      "Error 0.21114845309028518\n",
      "Error 0.2111448375568003\n",
      "Error 0.21114122482522987\n",
      "Error 0.2111376148919724\n",
      "Error 0.21113400775343324\n",
      "Error 0.21113040340602335\n",
      "Error 0.21112680184616095\n",
      "Error 0.21112320307027094\n",
      "Error 0.21111960707478428\n",
      "Error 0.2111160138561384\n",
      "Error 0.21111242341077724\n",
      "Error 0.21110883573515116\n",
      "Error 0.21110525082571724\n",
      "Error 0.21110166867893826\n",
      "Error 0.21109808929128446\n",
      "Error 0.21109451265923146\n",
      "Error 0.21109093877926166\n",
      "Error 0.21108736764786387\n",
      "Error 0.21108379926153314\n",
      "Error 0.21108023361677075\n",
      "Error 0.21107667071008462\n",
      "Error 0.21107311053798866\n",
      "Error 0.21106955309700298\n",
      "Error 0.2110659983836542\n",
      "Error 0.21106244639447552\n",
      "Error 0.21105889712600537\n",
      "Error 0.21105535057478897\n",
      "Error 0.21105180673737803\n",
      "Error 0.21104826561033002\n",
      "Error 0.21104472719020867\n",
      "Error 0.21104119147358422\n",
      "Error 0.21103765845703246\n",
      "Error 0.2110341281371359\n",
      "Error 0.21103060051048259\n",
      "Error 0.21102707557366704\n",
      "Error 0.2110235533232901\n",
      "Error 0.2110200337559581\n",
      "Error 0.21101651686828368\n",
      "Error 0.21101300265688558\n",
      "Error 0.21100949111838876\n",
      "Error 0.21100598224942382\n",
      "Error 0.21100247604662764\n",
      "Error 0.21099897250664285\n",
      "Error 0.21099547162611834\n",
      "Error 0.21099197340170908\n",
      "Error 0.2109884778300751\n",
      "Error 0.21098498490788348\n",
      "Error 0.21098149463180663\n",
      "Error 0.21097800699852293\n",
      "Error 0.21097452200471684\n",
      "Error 0.21097103964707845\n",
      "Error 0.21096755992230393\n",
      "Error 0.2109640828270952\n",
      "Error 0.21096060835816002\n",
      "Error 0.21095713651221204\n",
      "Error 0.21095366728597056\n",
      "Error 0.21095020067616085\n",
      "Error 0.21094673667951408\n",
      "Error 0.210943275292767\n",
      "Error 0.21093981651266222\n",
      "Error 0.21093636033594798\n",
      "Error 0.2109329067593782\n",
      "Error 0.21092945577971264\n",
      "Error 0.21092600739371722\n",
      "Error 0.21092256159816242\n",
      "Error 0.21091911838982558\n",
      "Error 0.21091567776548917\n",
      "Error 0.2109122397219413\n",
      "Error 0.21090880425597577\n",
      "Error 0.21090537136439208\n",
      "Error 0.21090194104399526\n",
      "Error 0.21089851329159606\n",
      "Error 0.2108950881040108\n",
      "Error 0.21089166547806143\n",
      "Error 0.21088824541057524\n",
      "Error 0.2108848278983852\n",
      "Error 0.21088141293833\n",
      "Error 0.21087800052725353\n",
      "Error 0.2108745906620054\n",
      "Error 0.2108711833394409\n",
      "Error 0.21086777855642044\n",
      "Error 0.21086437630981023\n",
      "Error 0.21086097659648134\n",
      "Error 0.21085757941331132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.21085418475718237\n",
      "Error 0.21085079262498244\n",
      "Error 0.2108474030136048\n",
      "Error 0.21084401591994806\n",
      "Error 0.21084063134091643\n",
      "Error 0.21083724927341932\n",
      "Error 0.2108338697143716\n",
      "Error 0.21083049266069326\n",
      "Error 0.21082711810931007\n",
      "Error 0.2108237460571532\n",
      "Error 0.21082037650115834\n",
      "Error 0.2108170094382673\n",
      "Error 0.21081364486542692\n",
      "Error 0.2108102827795892\n",
      "Error 0.21080692317771135\n",
      "Error 0.21080356605675651\n",
      "Error 0.2108002114136922\n",
      "Error 0.2107968592454914\n",
      "Error 0.21079350954913306\n",
      "Error 0.2107901623216005\n",
      "Error 0.21078681755988246\n",
      "Error 0.21078347526097266\n",
      "Error 0.2107801354218708\n",
      "Error 0.21077679803958066\n",
      "Error 0.21077346311111195\n",
      "Error 0.2107701306334797\n",
      "Error 0.21076680060370312\n",
      "Error 0.21076347301880743\n",
      "Error 0.21076014787582253\n",
      "Error 0.21075682517178382\n",
      "Error 0.21075350490373118\n",
      "Error 0.21075018706870985\n",
      "Error 0.21074687166377043\n",
      "Error 0.21074355868596822\n",
      "Error 0.21074024813236383\n",
      "Error 0.21073694000002258\n",
      "Error 0.21073363428601494\n",
      "Error 0.21073033098741673\n",
      "Error 0.21072703010130794\n",
      "Error 0.21072373162477462\n",
      "Error 0.2107204355549071\n",
      "Error 0.2107171418888008\n",
      "Error 0.21071385062355624\n",
      "Error 0.21071056175627856\n",
      "Error 0.21070727528407834\n",
      "Error 0.21070399120407055\n",
      "Error 0.21070070951337558\n",
      "Error 0.21069743020911832\n",
      "Error 0.2106941532884287\n",
      "Error 0.21069087874844167\n",
      "Error 0.21068760658629704\n",
      "Error 0.2106843367991389\n",
      "Error 0.2106810693841171\n",
      "Error 0.21067780433838545\n",
      "Error 0.21067454165910324\n",
      "Error 0.21067128134343438\n",
      "Error 0.21066802338854754\n",
      "Error 0.21066476779161616\n",
      "Error 0.21066151454981846\n",
      "Error 0.21065826366033763\n",
      "Error 0.21065501512036136\n",
      "Error 0.21065176892708215\n",
      "Error 0.21064852507769755\n",
      "Error 0.21064528356940934\n",
      "Error 0.21064204439942427\n",
      "Error 0.21063880756495407\n",
      "Error 0.2106355730632147\n",
      "Error 0.21063234089142696\n",
      "Error 0.21062911104681672\n",
      "Error 0.21062588352661402\n",
      "Error 0.2106226583280537\n",
      "Error 0.21061943544837508\n",
      "Error 0.21061621488482274\n",
      "Error 0.2106129966346455\n",
      "Error 0.21060978069509623\n",
      "Error 0.21060656706343361\n",
      "Error 0.21060335573691966\n",
      "Error 0.2106001467128221\n",
      "Error 0.21059693998841272\n",
      "Error 0.21059373556096767\n",
      "Error 0.2105905334277681\n",
      "Error 0.21058733358609943\n",
      "Error 0.21058413603325157\n",
      "Error 0.21058094076651934\n",
      "Error 0.21057774778320146\n",
      "Error 0.21057455708060177\n",
      "Error 0.2105713686560285\n",
      "Error 0.21056818250679393\n",
      "Error 0.2105649986302153\n",
      "Error 0.21056181702361437\n",
      "Error 0.21055863768431676\n",
      "Error 0.21055546060965336\n",
      "Error 0.21055228579695867\n",
      "Error 0.21054911324357217\n",
      "Error 0.21054594294683787\n",
      "Error 0.2105427749041036\n",
      "Error 0.2105396091127223\n",
      "Error 0.21053644557005075\n",
      "Error 0.21053328427345025\n",
      "Error 0.21053012522028683\n",
      "Error 0.2105269684079306\n",
      "Error 0.2105238138337556\n",
      "Error 0.2105206614951411\n",
      "Error 0.21051751138947009\n",
      "Error 0.2105143635141305\n",
      "Error 0.21051121786651353\n",
      "Error 0.21050807444401584\n",
      "Error 0.21050493324403768\n",
      "Error 0.2105017942639838\n",
      "Error 0.21049865750126306\n",
      "Error 0.2104955229532889\n",
      "Error 0.21049239061747893\n",
      "Error 0.2104892604912548\n",
      "Error 0.21048613257204268\n",
      "Error 0.21048300685727322\n",
      "Error 0.2104798833443804\n",
      "Error 0.2104767620308035\n",
      "Error 0.21047364291398507\n",
      "Error 0.2104705259913727\n",
      "Error 0.2104674112604174\n",
      "Error 0.21046429871857497\n",
      "Error 0.2104611883633051\n",
      "Error 0.21045808019207185\n",
      "Error 0.21045497420234294\n",
      "Error 0.21045187039159083\n",
      "Error 0.2104487687572917\n",
      "Error 0.21044566929692618\n",
      "Error 0.210442572007979\n",
      "Error 0.21043947688793865\n",
      "Error 0.21043638393429842\n",
      "Error 0.21043329314455458\n",
      "Error 0.21043020451620847\n",
      "Error 0.21042711804676545\n",
      "Error 0.2104240337337342\n",
      "Error 0.2104209515746283\n",
      "Error 0.2104178715669652\n",
      "Error 0.21041479370826596\n",
      "Error 0.21041171799605599\n",
      "Error 0.21040864442786464\n",
      "Error 0.2104055730012257\n",
      "Error 0.21040250371367622\n",
      "Error 0.2103994365627578\n",
      "Error 0.21039637154601606\n",
      "Error 0.21039330866099992\n",
      "Error 0.21039024790526328\n",
      "Error 0.2103871892763635\n",
      "Error 0.2103841327718617\n",
      "Error 0.21038107838932332\n",
      "Error 0.2103780261263176\n",
      "Error 0.21037497598041752\n",
      "Error 0.21037192794920037\n",
      "Error 0.21036888203024712\n",
      "Error 0.21036583822114277\n",
      "Error 0.21036279651947618\n",
      "Error 0.21035975692284006\n",
      "Error 0.21035671942883086\n",
      "Error 0.21035368403504928\n",
      "Error 0.2103506507390999\n",
      "Error 0.2103476195385907\n",
      "Error 0.21034459043113385\n",
      "Error 0.2103415634143452\n",
      "Error 0.21033853848584472\n",
      "Error 0.2103355156432559\n",
      "Error 0.21033249488420633\n",
      "Error 0.21032947620632703\n",
      "Error 0.2103264596072531\n",
      "Error 0.2103234450846237\n",
      "Error 0.21032043263608108\n",
      "Error 0.21031742225927202\n",
      "Error 0.21031441395184663\n",
      "Error 0.21031140771145881\n",
      "Error 0.21030840353576635\n",
      "Error 0.21030540142243082\n",
      "Error 0.2103024013691175\n",
      "Error 0.21029940337349515\n",
      "Error 0.2102964074332368\n",
      "Error 0.21029341354601877\n",
      "Error 0.21029042170952103\n",
      "Error 0.21028743192142757\n",
      "Error 0.210284444179426\n",
      "Error 0.2102814584812076\n",
      "Error 0.21027847482446696\n",
      "Error 0.21027549320690292\n",
      "Error 0.21027251362621788\n",
      "Error 0.21026953608011767\n",
      "Error 0.21026656056631193\n",
      "Error 0.2102635870825138\n",
      "Error 0.21026061562644008\n",
      "Error 0.2102576461958114\n",
      "Error 0.21025467878835163\n",
      "Error 0.21025171340178886\n",
      "Error 0.2102487500338543\n",
      "Error 0.21024578868228305\n",
      "Error 0.2102428293448134\n",
      "Error 0.21023987201918767\n",
      "Error 0.21023691670315153\n",
      "Error 0.2102339633944541\n",
      "Error 0.21023101209084846\n",
      "Error 0.2102280627900907\n",
      "Error 0.2102251154899411\n",
      "Error 0.21022217018816303\n",
      "Error 0.21021922688252342\n",
      "Error 0.21021628557079292\n",
      "Error 0.21021334625074545\n",
      "Error 0.21021040892015908\n",
      "Error 0.21020747357681455\n",
      "Error 0.2102045402184963\n",
      "Error 0.2102016088429925\n",
      "Error 0.21019867944809484\n",
      "Error 0.21019575203159852\n",
      "Error 0.21019282659130165\n",
      "Error 0.21018990312500674\n",
      "Error 0.21018698163051877\n",
      "Error 0.21018406210564655\n",
      "Error 0.21018114454820305\n",
      "Error 0.21017822895600338\n",
      "Error 0.21017531532686715\n",
      "Error 0.2101724036586166\n",
      "Error 0.21016949394907836\n",
      "Error 0.21016658619608145\n",
      "Error 0.2101636803974587\n",
      "Error 0.21016077655104617\n",
      "Error 0.21015787465468402\n",
      "Error 0.21015497470621486\n",
      "Error 0.21015207670348524\n",
      "Error 0.21014918064434482\n",
      "Error 0.21014628652664663\n",
      "Error 0.21014339434824722\n",
      "Error 0.21014050410700644\n",
      "Error 0.21013761580078744\n",
      "Error 0.21013472942745648\n",
      "Error 0.21013184498488363\n",
      "Error 0.21012896247094176\n",
      "Error 0.2101260818835077\n",
      "Error 0.2101232032204608\n",
      "Error 0.21012032647968398\n",
      "Error 0.21011745165906417\n",
      "Error 0.2101145787564905\n",
      "Error 0.21011170776985577\n",
      "Error 0.2101088386970565\n",
      "Error 0.21010597153599195\n",
      "Error 0.21010310628456486\n",
      "Error 0.21010024294068114\n",
      "Error 0.21009738150225013\n",
      "Error 0.210094521967184\n",
      "Error 0.21009166433339854\n",
      "Error 0.2100888085988128\n",
      "Error 0.21008595476134878\n",
      "Error 0.21008310281893156\n",
      "Error 0.21008025276949\n",
      "Error 0.21007740461095578\n",
      "Error 0.2100745583412639\n",
      "Error 0.21007171395835225\n",
      "Error 0.21006887146016223\n",
      "Error 0.21006603084463835\n",
      "Error 0.21006319210972818\n",
      "Error 0.2100603552533828\n",
      "Error 0.21005752027355604\n",
      "Error 0.21005468716820497\n",
      "Error 0.2100518559352897\n",
      "Error 0.21004902657277383\n",
      "Error 0.21004619907862382\n",
      "Error 0.2100433734508096\n",
      "Error 0.21004054968730346\n",
      "Error 0.2100377277860816\n",
      "Error 0.21003490774512304\n",
      "Error 0.21003208956240985\n",
      "Error 0.21002927323592696\n",
      "Error 0.21002645876366297\n",
      "Error 0.21002364614360922\n",
      "Error 0.21002083537376018\n",
      "Error 0.21001802645211315\n",
      "Error 0.21001521937666875\n",
      "Error 0.21001241414543093\n",
      "Error 0.21000961075640592\n",
      "Error 0.21000680920760378\n",
      "Error 0.2100040094970371\n",
      "Error 0.21000121162272173\n",
      "Error 0.20999841558267668\n",
      "Error 0.20999562137492359\n",
      "Error 0.20999282899748745\n",
      "Error 0.20999003844839606\n",
      "Error 0.20998724972568036\n",
      "Error 0.2099844628273741\n",
      "Error 0.20998167775151436\n",
      "Error 0.20997889449614088\n",
      "Error 0.20997611305929645\n",
      "Error 0.20997333343902716\n",
      "Error 0.20997055563338138\n",
      "Error 0.2099677796404114\n",
      "Error 0.20996500545817157\n",
      "Error 0.20996223308471945\n",
      "Error 0.20995946251811617\n",
      "Error 0.20995669375642478\n",
      "Error 0.20995392679771196\n",
      "Error 0.2099511616400469\n",
      "Error 0.2099483982815022\n",
      "Error 0.2099456367201528\n",
      "Error 0.20994287695407707\n",
      "Error 0.20994011898135617\n",
      "Error 0.20993736280007397\n",
      "Error 0.2099346084083173\n",
      "Error 0.20993185580417606\n",
      "Error 0.20992910498574266\n",
      "Error 0.2099263559511126\n",
      "Error 0.20992360869838436\n",
      "Error 0.2099208632256592\n",
      "Error 0.2099181195310412\n",
      "Error 0.20991537761263737\n",
      "Error 0.20991263746855746\n",
      "Error 0.20990989909691407\n",
      "Error 0.20990716249582272\n",
      "Error 0.20990442766340167\n",
      "Error 0.20990169459777208\n",
      "Error 0.20989896329705834\n",
      "Error 0.20989623375938674\n",
      "Error 0.2098935059828872\n",
      "Error 0.20989077996569186\n",
      "Error 0.20988805570593608\n",
      "Error 0.20988533320175762\n",
      "Error 0.20988261245129786\n",
      "Error 0.2098798934526998\n",
      "Error 0.20987717620410976\n",
      "Error 0.2098744607036771\n",
      "Error 0.20987174694955382\n",
      "Error 0.20986903493989445\n",
      "Error 0.20986632467285626\n",
      "Error 0.20986361614659965\n",
      "Error 0.20986090935928736\n",
      "Error 0.20985820430908494\n",
      "Error 0.2098555009941611\n",
      "Error 0.20985279941268656\n",
      "Error 0.2098500995628357\n",
      "Error 0.20984740144278474\n",
      "Error 0.20984470505071248\n",
      "Error 0.2098420103848017\n",
      "Error 0.20983931744323678\n",
      "Error 0.20983662622420493\n",
      "Error 0.20983393672589662\n",
      "Error 0.20983124894650426\n",
      "Error 0.20982856288422352\n",
      "Error 0.20982587853725249\n",
      "Error 0.20982319590379195\n",
      "Error 0.20982051498204504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.20981783577021815\n",
      "Error 0.20981515826652034\n",
      "Error 0.20981248246916265\n",
      "Error 0.20980980837635946\n",
      "Error 0.2098071359863276\n",
      "Error 0.209804465297286\n",
      "Error 0.20980179630745704\n",
      "Error 0.209799129015065\n",
      "Error 0.2097964634183374\n",
      "Error 0.20979379951550398\n",
      "Error 0.20979113730479731\n",
      "Error 0.20978847678445262\n",
      "Error 0.20978581795270723\n",
      "Error 0.20978316080780185\n",
      "Error 0.209780505347979\n",
      "Error 0.20977785157148426\n",
      "Error 0.20977519947656562\n",
      "Error 0.20977254906147386\n",
      "Error 0.2097699003244621\n",
      "Error 0.20976725326378626\n",
      "Error 0.2097646078777046\n",
      "Error 0.20976196416447804\n",
      "Error 0.20975932212236964\n",
      "Error 0.20975668174964573\n",
      "Error 0.20975404304457496\n",
      "Error 0.20975140600542827\n",
      "Error 0.20974877063047911\n",
      "Error 0.20974613691800373\n",
      "Error 0.20974350486628096\n",
      "Error 0.20974087447359185\n",
      "Error 0.2097382457382199\n",
      "Error 0.20973561865845158\n",
      "Error 0.20973299323257572\n",
      "Error 0.20973036945888293\n",
      "Error 0.20972774733566774\n",
      "Error 0.20972512686122585\n",
      "Error 0.20972250803385584\n",
      "Error 0.20971989085185908\n",
      "Error 0.20971727531353923\n",
      "Error 0.20971466141720238\n",
      "Error 0.20971204916115718\n"
     ]
    }
   ],
   "source": [
    "X_list = list(np.random.uniform(size=(100)))\n",
    "y_list = list(map( lambda m: 0.0 if (m >= 0.46 and m <= 0.54) else 1, X))\n",
    "data = list(zip(X_list, y_list))\n",
    "train = data[0:80]\n",
    "test = data[80:]\n",
    "epochs = 2000\n",
    "lr =0.2\n",
    "for j in range(epochs):\n",
    "    error = []\n",
    "    for x,y in train:\n",
    "        G.add_node(\"a_0\",weight=x)\n",
    "        G.add_node(\"a_1\",weight=fn_sigmoid(w_0_1*G.nodes[\"a_0\"][\"weight\"] + b_1))\n",
    "        G.add_node(\"a_2\",weight=fn_sigmoid(w_0_2*G.nodes[\"a_0\"][\"weight\"] + b_2))\n",
    "        G.add_node(\"a_3\",weight=fn_sigmoid(w_2_3*G.nodes[\"a_2\"][\"weight\"]+\n",
    "                                           w_1_3*G.nodes[\"a_1\"][\"weight\"]+ b_3))\n",
    "        G.add_edge(\"a_0\",\"a_1\",weight=w_0_1)\n",
    "        G.add_edge(\"a_0\",\"a_2\",weight=w_0_2)\n",
    "        G.add_edge(\"a_2\",\"a_3\",weight=w_2_3)\n",
    "        G.add_edge(\"a_1\",\"a_3\",weight=w_1_3)\n",
    "        \n",
    "        w_1_3 -= lr*(2/2*(G.nodes[\"a_3\"][\"weight\"]-y) *G.nodes[\"a_3\"][\"weight\"]*(1-G.nodes[\"a_3\"][\"weight\"])*G.nodes[\"a_1\"][\"weight\"])\n",
    "        w_2_3 -= lr*(2/2*(G.nodes[\"a_3\"][\"weight\"]-y) *G.nodes[\"a_3\"][\"weight\"]*(1-G.nodes[\"a_3\"][\"weight\"])*G.nodes[\"a_2\"][\"weight\"])\n",
    "\n",
    "        w_0_2 -= lr*(2/2*(G.nodes[\"a_3\"][\"weight\"]-y) \\\n",
    "                * G.nodes[\"a_3\"][\"weight\"]*(1-G.nodes[\"a_3\"][\"weight\"])*w_2_3\\\n",
    "                * (G.nodes[\"a_2\"][\"weight\"])*(1- G.nodes[\"a_2\"][\"weight\"])*x)\n",
    "        w_0_1 -= lr*(2/2*(G.nodes[\"a_3\"][\"weight\"]-y)\\\n",
    "                *G.nodes[\"a_3\"][\"weight\"]*(1-G.nodes[\"a_3\"][\"weight\"])*w_1_3 \\\n",
    "                * (G.nodes[\"a_1\"][\"weight\"])*(1- G.nodes[\"a_1\"][\"weight\"])*x)\n",
    "        #print(abs(G.nodes[\"a_3\"][\"weight\"] - y),w_1_3,w_2_3,w_0_2,w_0_1)\n",
    "        error.append(abs(G.nodes[\"a_3\"][\"weight\"] - y))\n",
    "        \n",
    "    print(\"Error %s\" % (sum(error)/len(error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pero la vida real es dificil!!\n",
    "No tenemos la funcion que intentamos predecir. Solamente contamos con datos de entrada y etiquetas. Las etiquetas nos ayudan a saber si vamos bien o mal.\n",
    "\n",
    "Para aprender tenemos:\n",
    "- Si es un problema de clasificacion: Cross entropy.\n",
    "Ver notebook [arboles](https://jdramirez.github.io/UCO_ML_AI/Trees.html)\n",
    "\n",
    "${\\displaystyle H(p,q)=-\\sum _{x\\in {\\mathcal {X}}}p(x)\\,\\log q(x)} =\\ -y\\log {\\hat  {y}}-(1-y)\\log(1-{\\hat  {y}})$\n",
    "\n",
    "from http://wiki.fast.ai/index.php/Log_Loss\n",
    "\n",
    "\n",
    "\n",
    "- Si es regresion MSE.\n",
    "\n",
    "${\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from  https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "def mean_squared_error(actual, predicted):\n",
    "    sum_square_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "    mean_square_error = 1.0 / len(actual) * sum_square_error\n",
    "    return mean_square_error\n",
    "\n",
    "from math import log\n",
    " \n",
    "# calculate binary cross entropy\n",
    "def binary_cross_entropy(actual, predicted):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_score += actual[i] * log(1e-15 + predicted[i])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    return -mean_sum_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "# Taken from https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    " \n",
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "    \n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    " \n",
    "# Test making predictions with the network\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n",
    "    [{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Programacion Simbolica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-((((-(fill((TensorConstant{1} / (TensorConstant{1} + exp(((-x) * TensorConstant{0.21062904250930625})))), TensorConstant{1.0}) * TensorConstant{1})) / ((TensorConstant{1} + exp(((-x) * TensorConstant{0.21062904250930625}))) * (TensorConstant{1} + exp(((-x) * TensorConstant{0.21062904250930625}))))) * exp(((-x) * TensorConstant{0.21062904250930625}))) * TensorConstant{0.21062904250930625}))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.04431657)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from theano import pp\n",
    "x = T.dscalar('x')\n",
    "w = np.random.uniform()\n",
    "y = 1/(1+ np.exp(-x*w))\n",
    "\n",
    "gy = T.grad(y, x)\n",
    "print(pp(gy))  # print out the gradient prior to optimization\n",
    "f = theano.function([x], gy)\n",
    "f(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... training the model\n",
      "epoch 1, minibatch 83/83, validation error 12.458333 %\n",
      "     epoch 1, minibatch 83/83, test error of best model 12.375000 %\n",
      "epoch 2, minibatch 83/83, validation error 11.010417 %\n",
      "     epoch 2, minibatch 83/83, test error of best model 10.958333 %\n",
      "epoch 3, minibatch 83/83, validation error 10.312500 %\n",
      "     epoch 3, minibatch 83/83, test error of best model 10.312500 %\n",
      "epoch 4, minibatch 83/83, validation error 9.875000 %\n",
      "     epoch 4, minibatch 83/83, test error of best model 9.833333 %\n",
      "epoch 5, minibatch 83/83, validation error 9.562500 %\n",
      "     epoch 5, minibatch 83/83, test error of best model 9.479167 %\n",
      "epoch 6, minibatch 83/83, validation error 9.322917 %\n",
      "     epoch 6, minibatch 83/83, test error of best model 9.291667 %\n",
      "epoch 7, minibatch 83/83, validation error 9.187500 %\n",
      "     epoch 7, minibatch 83/83, test error of best model 9.000000 %\n",
      "epoch 8, minibatch 83/83, validation error 8.989583 %\n",
      "     epoch 8, minibatch 83/83, test error of best model 8.958333 %\n",
      "epoch 9, minibatch 83/83, validation error 8.937500 %\n",
      "     epoch 9, minibatch 83/83, test error of best model 8.812500 %\n",
      "epoch 10, minibatch 83/83, validation error 8.750000 %\n",
      "     epoch 10, minibatch 83/83, test error of best model 8.666667 %\n",
      "epoch 11, minibatch 83/83, validation error 8.666667 %\n",
      "     epoch 11, minibatch 83/83, test error of best model 8.520833 %\n",
      "epoch 12, minibatch 83/83, validation error 8.583333 %\n",
      "     epoch 12, minibatch 83/83, test error of best model 8.416667 %\n",
      "epoch 13, minibatch 83/83, validation error 8.489583 %\n",
      "     epoch 13, minibatch 83/83, test error of best model 8.291667 %\n",
      "epoch 14, minibatch 83/83, validation error 8.427083 %\n",
      "     epoch 14, minibatch 83/83, test error of best model 8.281250 %\n",
      "epoch 15, minibatch 83/83, validation error 8.354167 %\n",
      "     epoch 15, minibatch 83/83, test error of best model 8.270833 %\n",
      "epoch 16, minibatch 83/83, validation error 8.302083 %\n",
      "     epoch 16, minibatch 83/83, test error of best model 8.239583 %\n",
      "epoch 17, minibatch 83/83, validation error 8.250000 %\n",
      "     epoch 17, minibatch 83/83, test error of best model 8.177083 %\n",
      "epoch 18, minibatch 83/83, validation error 8.229167 %\n",
      "     epoch 18, minibatch 83/83, test error of best model 8.062500 %\n",
      "epoch 19, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 20, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 21, minibatch 83/83, validation error 8.208333 %\n",
      "     epoch 21, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 22, minibatch 83/83, validation error 8.187500 %\n",
      "     epoch 22, minibatch 83/83, test error of best model 7.927083 %\n",
      "epoch 23, minibatch 83/83, validation error 8.156250 %\n",
      "     epoch 23, minibatch 83/83, test error of best model 7.958333 %\n",
      "epoch 24, minibatch 83/83, validation error 8.114583 %\n",
      "     epoch 24, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 25, minibatch 83/83, validation error 8.093750 %\n",
      "     epoch 25, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 26, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 27, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 28, minibatch 83/83, validation error 8.052083 %\n",
      "     epoch 28, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 29, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 30, minibatch 83/83, validation error 8.031250 %\n",
      "     epoch 30, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 31, minibatch 83/83, validation error 8.010417 %\n",
      "     epoch 31, minibatch 83/83, test error of best model 7.833333 %\n",
      "epoch 32, minibatch 83/83, validation error 7.979167 %\n",
      "     epoch 32, minibatch 83/83, test error of best model 7.812500 %\n",
      "epoch 33, minibatch 83/83, validation error 7.947917 %\n",
      "     epoch 33, minibatch 83/83, test error of best model 7.739583 %\n",
      "epoch 34, minibatch 83/83, validation error 7.875000 %\n",
      "     epoch 34, minibatch 83/83, test error of best model 7.729167 %\n",
      "epoch 35, minibatch 83/83, validation error 7.885417 %\n",
      "epoch 36, minibatch 83/83, validation error 7.843750 %\n",
      "     epoch 36, minibatch 83/83, test error of best model 7.697917 %\n",
      "epoch 37, minibatch 83/83, validation error 7.802083 %\n",
      "     epoch 37, minibatch 83/83, test error of best model 7.635417 %\n",
      "epoch 38, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 39, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 40, minibatch 83/83, validation error 7.822917 %\n",
      "epoch 41, minibatch 83/83, validation error 7.791667 %\n",
      "     epoch 41, minibatch 83/83, test error of best model 7.625000 %\n",
      "epoch 42, minibatch 83/83, validation error 7.770833 %\n",
      "     epoch 42, minibatch 83/83, test error of best model 7.614583 %\n",
      "epoch 43, minibatch 83/83, validation error 7.750000 %\n",
      "     epoch 43, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 44, minibatch 83/83, validation error 7.739583 %\n",
      "     epoch 44, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 45, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 46, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 47, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 48, minibatch 83/83, validation error 7.708333 %\n",
      "     epoch 48, minibatch 83/83, test error of best model 7.583333 %\n",
      "epoch 49, minibatch 83/83, validation error 7.677083 %\n",
      "     epoch 49, minibatch 83/83, test error of best model 7.572917 %\n",
      "epoch 50, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 51, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 52, minibatch 83/83, validation error 7.656250 %\n",
      "     epoch 52, minibatch 83/83, test error of best model 7.541667 %\n",
      "epoch 53, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 54, minibatch 83/83, validation error 7.635417 %\n",
      "     epoch 54, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 55, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 56, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 57, minibatch 83/83, validation error 7.604167 %\n",
      "     epoch 57, minibatch 83/83, test error of best model 7.489583 %\n",
      "epoch 58, minibatch 83/83, validation error 7.583333 %\n",
      "     epoch 58, minibatch 83/83, test error of best model 7.458333 %\n",
      "epoch 59, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 59, minibatch 83/83, test error of best model 7.468750 %\n",
      "epoch 60, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 61, minibatch 83/83, validation error 7.583333 %\n",
      "epoch 62, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 62, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 63, minibatch 83/83, validation error 7.562500 %\n",
      "     epoch 63, minibatch 83/83, test error of best model 7.510417 %\n",
      "epoch 64, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 65, minibatch 83/83, validation error 7.562500 %\n",
      "epoch 66, minibatch 83/83, validation error 7.552083 %\n",
      "     epoch 66, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 67, minibatch 83/83, validation error 7.552083 %\n",
      "epoch 68, minibatch 83/83, validation error 7.531250 %\n",
      "     epoch 68, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 69, minibatch 83/83, validation error 7.531250 %\n",
      "epoch 70, minibatch 83/83, validation error 7.510417 %\n",
      "     epoch 70, minibatch 83/83, test error of best model 7.500000 %\n",
      "epoch 71, minibatch 83/83, validation error 7.520833 %\n",
      "epoch 72, minibatch 83/83, validation error 7.510417 %\n",
      "epoch 73, minibatch 83/83, validation error 7.500000 %\n",
      "     epoch 73, minibatch 83/83, test error of best model 7.489583 %\n",
      "Optimization complete with best validation score of 7.500000 %,with test performance 7.489583 %\n",
      "The code run for 74 epochs, with 1.998166 epochs/sec\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9b8442090a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0msgd_optimization_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-9b8442090a41>\u001b[0m in \u001b[0;36msgd_optimization_mnist\u001b[0;34m(learning_rate, n_epochs, dataset, batch_size)\u001b[0m\n\u001b[1;32m    443\u001b[0m         epoch, 1. * epoch / (end_time - start_time)))\n\u001b[1;32m    444\u001b[0m     print(('The code for file ' +\n\u001b[0;32m--> 445\u001b[0;31m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m            ' ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#Taken from http://deeplearning.net/tutorial/logreg.html#logreg\n",
    "\"\"\"\n",
    "This tutorial introduces logistic regression using Theano and stochastic\n",
    "gradient descent.\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(__file__)[0],\n",
    "            \"..\",\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        )\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "    print('... loading data')\n",
    "\n",
    "    # Load the dataset\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # train_set, valid_set, test_set format: tuple(input, target)\n",
    "    # input is a numpy.ndarray of 2 dimensions (a matrix)\n",
    "    # where each row corresponds to an example. target is a\n",
    "    # numpy.ndarray of 1 dimension (vector) that has the same length as\n",
    "    # the number of rows in the input. It should give the target\n",
    "    # to the example with the same index in the input.\n",
    "\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.13, n_epochs=1000,\n",
    "                           dataset='data/mnist.pkl.gz',\n",
    "                           batch_size=600):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a\n",
    "    # minibatch)\n",
    "    x = T.matrix('x')  # data, presented as rasterized images\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # construct the logistic regression class\n",
    "    # Each MNIST image has size 28*28\n",
    "    classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # start-snippet-3\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-3\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "\n",
    "                    test_losses = [test_model(i)\n",
    "                                   for i in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%,'\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., test_score * 100.)\n",
    "    )\n",
    "    print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "        epoch, 1. * epoch / (end_time - start_time)))\n",
    "    print(('The code for file ' +\n",
    "           os.path.split(__file__)[1] +\n",
    "           ' ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)\n",
    "\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    An example of how to load a trained model and use it\n",
    "    to predict labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the saved model\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # We can test it on some examples from test test\n",
    "    dataset='mnist.pkl.gz'\n",
    "    datasets = load_data(dataset)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "\n",
    "    predicted_values = predict_model(test_set_x[:10])\n",
    "    print(\"Predicted values for the first 10 examples in test set:\")\n",
    "    print(predicted_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sgd_optimization_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
